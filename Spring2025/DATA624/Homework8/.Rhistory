geom_point()+
geom_hline(yintercept = 0, linetype = "dashed") +
labs(x = "Predicted",y ="Residuals")
## Checking for Linearity
ggplot(model, aes(x=.fitted, y=.resid)) +
geom_point()+
geom_hline(yintercept = 0, linetype = "dashed") +
labs(x = "Predicted",y ="Residuals")
# Residual Dist.
ggplot(data = model, aes(x = .resid)) +geom_histogram(binwidth = 1) + xlab("Residuals")
#Variability of Constant
ggplot(data = model, aes(sample = .resid)) +stat_qq()
model <- lm(Marks ~ time_study, data = data)
print(summary(model))
# Residual Dist.
ggplot(data = model, aes(x = .resid)) +geom_histogram(binwidth = 1) + xlab("Residuals")
#Variability of Constant
ggplot(data = model, aes(sample = .resid)) +stat_qq()
# Residual Dist.
ggplot(data = model, aes(x = .resid)) +geom_histogram(binwidth = 5) + xlab("Residuals")
# Residual Dist.
ggplot(data = model, aes(x = .resid)) +geom_histogram(binwidth = .5) + xlab("Residuals")
# Residual Dist.
ggplot(data = model, aes(x = .resid)) +geom_histogram(binwidth = 2) + xlab("Residuals")
## Exploratory GGPairs plot of the data to scope out relationships between variables.
p = ggpairs(data,progress = FALSE) +theme_minimal(base_size=9) +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
strip.text.x = element_text(angle = 90, hjust = 1),
strip.text.y = element_text(angle = 0, hjust = 1))
ggsave("ggpairs_plot.png", plot = p)
p <- ggplot(data = data, aes(x = Marks, y = time_study)) +
geom_point() +
stat_smooth(method = "lm", se = FALSE)+
labs(x="Average Time Spent by Student Studying (Hours)",
y="Marks Received by Student",
)
ggsave("linear_model.png", plot = p)
## Checking the Residuals with Secondary Plots
p<-ggplot(model, aes(x=.fitted, y=.resid)) +
geom_point()+
geom_hline(yintercept = 0, linetype = "dashed") +
labs(x = "Predicted",y ="Residuals")
ggsave("residuals1.png", plot = p)
# Residual Dist.
p<-ggplot(data = model, aes(x = .resid)) +geom_histogram(binwidth = 2) + xlab("Residuals")
ggsave("residuals2.png", plot = p)
#Variability of Constant
p<-ggplot(data = model, aes(sample = .resid)) +stat_qq()
ggsave("residuals3.png", plot = p)
## Gettign a look at the data
print(head(data))
print(nrow(data))
p <- ggplot(data = data, aes(x = Marks, y = time_study)) +
geom_point() +
stat_smooth(method = "lm", se = FALSE)+
labs(x="Average Time Spent by Student Studying",
y="Marks Received by Student",
)
ggsave("linear_model.png", plot = p)
library(caret)
#library(MASS)
library(mlbench)
set.seed(200)
trainingData <- mlbench.friedman1(200, sd = 1)
## We convert the 'x' data from a matrix to a data frame
## One reason is that this will give the columns names.
trainingData$x <- data.frame(trainingData$x)
## Look at the data using
featurePlot(trainingData$x, trainingData$y)
## or other methods.
## This creates a list with a vector 'y' and a matrix
## of predictors 'x'. Also simulate a large test set to
## estimate the true error rate with good precision:
testData <- mlbench.friedman1(5000, sd = 1)
testData$x <- data.frame(testData$x)
library(caret)
knnModel <- train(x = trainingData$x,
+ y = trainingData$y,
knnModel <- train(x = trainingData$x,
y = trainingData$y,method = "knn",
preProc = c("center", "scale"),
tuneLength = 10)
knnModel
knnPred <- predict(knnModel, newdata = testData$x)
## The function 'postResample' can be used to get the test set
## perforamnce values
postResample(pred = knnPred, obs = testData$y)
## Look at the data using
print(featurePlot(trainingData$x, trainingData$y))
library(AppliedPredictiveModeling)
data(chemicalManufacturing)
library(caret)
library(earth)
install.packages('earth')
library(earth)
## Setting up the parameters for the MARS model allowing individual predictors, as well as limited dual combinations of them. Allowing large range for pruning.
marsGrid <- expand.grid(.degree = 1:2,  .nprune = 2:50)
marsFit <- train(x = trainingData[, predictors],
y = trainingData$target,
method = "earth",
tuneGrid = marsGrid,
preProc = c("center", "scale"))
marsFit <- train(x = trainingData$x,
y = trainingData$target,
method = "earth",
tuneGrid = marsGrid,
preProc = c("center", "scale"))
## Look at the data using
print(featurePlot(trainingData$x, trainingData$y))
print(trainingData)
## Setting up the parameters for the MARS model allowing individual predictors, as well as limited dual combinations of them. Allowing large range for pruning.
marsGrid <- expand.grid(.degree = 1:2,  .nprune = 2:50)
svmFit <- train(x = trainingData[, predictors],
y = trainingData$target,
method = "svmRadial",
tuneGrid = svmGrid,
preProc = c("center", "scale"))
marsFit <- train(x = trainingData$x,
y = trainingData$target,
method = "earth",
tuneGrid = marsGrid,
preProc = c("center", "scale"))
trainingData$x
marsFit <- train(x = trainingData$x,
y = trainingData$target,
method = "earth",
tuneGrid = marsGrid,
preProc = c("center", "scale"))
marsFit <- train(x = trainingData$x,
y = trainingData$y,
method = "earth",
tuneGrid = marsGrid,
preProc = c("center", "scale"))
summary(marsFit$finalModel)
#Predictions
marsPred <- predict(marsModel, newdata = testData$x)
#Predictions
marsPred <- predict(marsFit, newdata = testData$x)
print(postResample(pred = marsPred, obs = testData$y))
### Neural net model, we have 1-10 neurons, keeping inline with text books decay and bagging suggestions,
netGrid <- expand.grid(.decay = c(0, 0.01, 0.1),.size = 1:10,.bag = FALSE)
nnetTune <- train(trainingData$x, trainingData$y,method = "avNNet",  tuneGrid = nnetGrid,
trControl = ctrl,
preProc = c("center", "scale"),
linout = TRUE,
trace = FALSE,
MaxNWts = 10 * (ncol(trainXnnet) + 1) + 10 + 1,
maxit = 500)
nnetTune <- train(trainingData$x, trainingData$y,method = "avNNet",  tuneGrid = nnetGrid,
trControl = ctrl,
preProc = c("center", "scale"),
linout = TRUE,
trace = FALSE,
MaxNWts = 10 * (ncol(trainXnnet) + 1) + 10 + 1,
maxit = 500)
nnetTune <- train(trainingData$x, trainingData$y,method = "avNNet",  tuneGrid = netGrid,
trControl = ctrl,
preProc = c("center", "scale"),
linout = TRUE,
trace = FALSE,
MaxNWts = 10 * (ncol(trainXnnet) + 1) + 10 + 1,
maxit = 500)
nnetTune <- train(trainingData$x, trainingData$y,method = "avNNet",  tuneGrid = netGrid,
trControl = ctrl,
preProc = c("center", "scale"),
linout = TRUE,
trace = FALSE,maxit = 500)
print(summary(nnetTune))
print(summary(nnetTune$bestTune))
print(nnetTune$results)
print(nnetTune$results)
plot(nnetTune)
print(postResample(pred = nnetPred, obs = testData$y))
### Executing on Test Data
nnetPred <- predict(nnetTune, newdata = testData$x)
print(postResample(pred = nnetPred, obs = testData$y))
marsFit <- train(x = trainingData$x,
y = trainingData$y,
method = "earth",
tuneGrid = marsGrid,
preProc = c("center", "scale"),
trControl = trainControl(method = "cv"))
print(summary(marsFit$finalModel))
#Predictions
marsPred <- predict(marsFit, newdata = testData$x)
print(postResample(pred = marsPred, obs = testData$y))
svmRTuned <- train(trainingData$x, trainingData$y,
method = "svmRadial",
preProc = c("center", "scale"),
tuneLength = 14,
trControl = trainControl(method = "cv"))
svmRTuned <- train(trainingData$x, trainingData$y,
method = "svmRadial",
preProc = c("center", "scale"),
tuneLength = 14,
trControl = trainControl(method = "cv"))
library(kernlab)
library(kernlab)
svmRTuned <- train(trainingData$x, trainingData$y,
method = "svmRadial",
preProc = c("center", "scale"),
tuneLength = 14,
trControl = trainControl(method = "cv"))
svmRTuned$finalModel
print(summary(svmRTuned$finalModel))
print(svmRTuned$results)
print(summary(svmRTuned$bestTune))
print(svmRTuned$results)
plot(svmRTuned)
## Test Data
vmPred <- predict(svmRTuned, newdata = testData$x)
postResample(pred = svmPred, obs = testData$y)
## Test Data
svmPred <- predict(svmRTuned, newdata = testData$x)
postResample(pred = svmPred, obs = testData$y)
set.seed(100)
svmRTuned <- train(trainingData$x, trainingData$y,
method = "svmRadial",
preProc = c("center", "scale"),
tuneLength = 14,
trControl = trainControl(method = "cv"))
svmRTuned <- train(trainingData$x, trainingData$y,
method = "svmRadial",
preProc = c("center", "scale"),
tuneLength = 14,
trControl = trainControl(method = "cv"))
print(summary(svmRTuned$finalModel))
print(svmRTuned$results)
## Test Data
svmPred <- predict(svmRTuned, newdata = testData$x)
postResample(pred = svmPred, obs = testData$y)
## Test Data
svmPred <- predict(svmRTuned, newdata = testData$x)
postResample(pred = svmPred, obs = testData$y)
library(caret)
library(earth)
library(kernlab)
library(mlbench)
set.seed(200)
trainingData <- mlbench.friedman1(200, sd = 1)
## We convert the 'x' data from a matrix to a data frame One reason is that this will give the columns names.
$x <- data.frame(trainingData$x)
## We convert the 'x' data from a matrix to a data frame One reason is that this will give the columns names.
trainingData$x <- data.frame(trainingData$x)
## Look at the data using
print(featurePlot(trainingData$x, trainingData$y))
## or other methods.
## This creates a list with a vector 'y' and a matrix of predictors 'x'. Also simulate a large test set to estimate the true error rate with good precision:
testData <- mlbench.friedman1(5000, sd = 1)
testData$x <- data.frame(testData$x)
## or other methods.
## This creates a list with a vector 'y' and a matrix of predictors 'x'. Also simulate a large test set to estimate the true error rate with good precision:
testData <- mlbench.friedman1(5000, sd = 1)
testData$x <- data.frame(testData$x)
library(caret)
knnModel <- train(x = trainingData$x,
y = trainingData$y,
method = "knn",
preProc = c("center", "scale"),
tuneLength = 10)
knnModel <- train(x = trainingData$x,
y = trainingData$y,
method = "knn",
preProc = c("center", "scale"),
tuneLength = 10)
knnModel
knnPred <- predict(knnModel, newdata = testData$x)
## The function 'postResample' can be used to get the test set
## perforamnce values
postResample(pred = knnPred, obs = testData$y)
## Setting up the parameters for the MARS model allowing individual predictors, as well as limited dual combinations of them. Allowing large range for pruning.
marsGrid <- expand.grid(.degree = 1:2,  .nprune = 2:50)
set.seed(100)
marsFit <- train(x = trainingData$x,
y = trainingData$y,
method = "earth",
tuneGrid = marsGrid,
preProc = c("center", "scale"),
trControl = trainControl(method = "cv"))
marsFit <- train(x = trainingData$x,
y = trainingData$y,
method = "earth",
tuneGrid = marsGrid,
preProc = c("center", "scale"),
trControl = trainControl(method = "cv"))
print(summary(marsFit$finalModel))
#Predictions
marsPred <- predict(marsFit, newdata = testData$x)
print(postResample(pred = marsPred, obs = testData$y))
### Neural net model, we have 1-10 neurons, keeping inline with text books decay and bagging suggestions,
set.seed(100)
netGrid <- expand.grid(.decay = c(0, 0.01, 0.1),.size = 1:10,.bag = FALSE)
nnetTune <- train(trainingData$x, trainingData$y,method = "avNNet",  tuneGrid = netGrid,
trControl = ctrl,
preProc = c("center", "scale"),
linout = TRUE,
trace = FALSE,maxit = 500)
nnetTune <- train(trainingData$x, trainingData$y,method = "avNNet",  tuneGrid = netGrid,
trControl = trainControl(method = "cv"),
preProc = c("center", "scale"),
linout = TRUE,
trace = FALSE,maxit = 500)
print(summary(nnetTune$bestTune))
print(summary(nnetTune$finalModel))
print(nnetTune$results)
print(plot(nnetTune))
### Executing on Test Data
nnetPred <- predict(nnetTune, newdata = testData$x)
print(postResample(pred = nnetPred, obs = testData$y))
set.seed(100)
svmRTuned <- train(trainingData$x, trainingData$y,
method = "svmRadial",
preProc = c("center", "scale"),
tuneLength = 14,
trControl = trainControl(method = "cv"))
print(summary(svmRTuned$finalModel))
print(svmRTuned$results)
## Test Data
svmPred <- predict(svmRTuned, newdata = testData$x)
postResample(pred = svmPred, obs = testData$y)
library(AppliedPredictiveModeling)
data(chemicalManufacturing)
data(ChemicalManufacturingProcess)
ChemicalManufacturingProcess
ChemicalManufacturingProcess$chemicals
sample <- sample.int(n = nrow(ChemicalManufacturingProcess), size = floor(.75*nrow(ChemicalManufacturingProcess)), replace = F)
train <- ChemicalManufacturingProcess[sample, ]
test  <- ChemicalManufacturingProcess[-sample, ]
trainingData$x <- data.frame(ChemicalManufacturingProcess$x)
trainingData$x <- data.frame(ChemicalManufacturingProcess$x)
print(featurePlot(trainingData$x, trainingData$y))
## Establishing the x and y vars
X <- ChemicalManufacturingProcess[, -which(names(ChemicalManufacturingProcess) == "Yield")]
y <- ChemicalManufacturingProcess$Yield
## putting together for working & spltting
ChemData <- list(x = X, y = y)
ChemData
## Splitting 70/30
splitIndex <- createDataPartition(ChemData$y, p = 0.7, list = FALSE)
trainingData <- list(x = data.frame(fullData$x[splitIndex, ]),y = fullData$y[splitIndex])
ChemtrainingData <- list(x = data.frame(ChemData$x[splitIndex, ]),y = ChemData$y[splitIndex])
ChemtestData <- list( x = data.frame(ChemData$x[-splitIndex, ]),y = ChemData$y[-splitIndex])
## Just checking for good measure
##FUll
print(nrow(ChemicalManufacturingProcess))
##Train
print(nrow(ChemtrainingData))
##Test
print(nrow(ChemtestData))
ChemtestData
##Train
print(len(ChemtrainingData))
##Train
print(length(ChemtrainingData))
##Train
print(ChemtrainingData)
ChemtrainingData <- list(x = data.frame(ChemData$x[splitIndex, ]),y = ChemData$y[splitIndex])
ChemtestData <- list( x = data.frame(ChemData$x[-splitIndex, ]),y = ChemData$y[-splitIndex])
## IMPUTING Keeping it simple with taking the median values of each of the columns, as most columns that have NA only have one NA
imputed <- preProcess(ChemicalManufacturingProcess, method = "medianImpute")
chem_df_imputed <- predict(imputed, chem_df)
chem_df_imputed <- predict(imputed, ChemicalManufacturingProcess)
## Establishing the x and y vars
X <- chem_df_imputed[, -which(names(chem_df_imputed) == "Yield")]
y <- chem_df_imputed$Yield
## putting together for working & spltting
ChemData <- list(x = X, y = y)
## Splitting 70/30
splitIndex <- createDataPartition(ChemData$y, p = 0.7, list = FALSE)
ChemtrainingData <- list(x = data.frame(ChemData$x[splitIndex, ]),y = ChemData$y[splitIndex])
ChemtestData <- list( x = data.frame(ChemData$x[-splitIndex, ]),y = ChemData$y[-splitIndex])
#Looking at which of the predictor variables create the best fitting non-linear model to predict yield
set.seed(100)
### Firstly the Neural Net Model
netGrid <- expand.grid(.decay = c(0, 0.01, 0.1),.size = 1:10,.bag = FALSE)
nnetTune <- train( ChemtrainingData$X,
ChemtrainingData$Y,
method = "avNNet",  tuneGrid = netGrid,
trControl = trainControl(method = "cv"),
preProc = c("center", "scale"),
linout = TRUE,
trace = FALSE,maxit = 500)
nnetTune <- train( ChemtrainingData$x,
ChemtrainingData$y,
method = "avNNet",  tuneGrid = netGrid,
trControl = trainControl(method = "cv"),
preProc = c("center", "scale"),
linout = TRUE,
trace = FALSE,maxit = 500)
nnetTune <- train( ChemtrainingData$x,
ChemtrainingData$y,
method = "avNNet",  tuneGrid = netGrid,
trControl = trainControl(method = "cv"),
preProc = c("center", "scale"),
linout = TRUE,
trace = FALSE,maxit = 500)
#----Evaluation
nnetPred <- predict(nnetTune, newdata = ChemtestData$X)
print(postResample(pred = nnetPred, obs = ChemtestData$Y))
### Firstly the Neural Net Model
netGrid <- expand.grid(.decay = c(0, 0.01, 0.1),.size = 1:10,.bag = FALSE)
nnetTune <- train( ChemtrainingData$x,
ChemtrainingData$y,
method = "avNNet",  tuneGrid = netGrid,
trControl = trainControl(method = "cv"),
preProc = c("center", "scale"),
linout = TRUE,
trace = FALSE,maxit = 500)
nnetTune <- train( ChemtrainingData$x,
ChemtrainingData$y,
method = "avNNet",  tuneGrid = netGrid,
trControl = trainControl(method = "cv"),
preProc = c("center", "scale"),
linout = TRUE,
trace = FALSE,maxit = 500)
library(caret)
library(earth)
library(kernlab)
#Looking at which of the predictor variables create the best fitting non-linear model to predict yield
set.seed(100)
##SVM model
svmRTuned <- train(ChemtrainingData$x, hemtrainingData$y,
method = "svmRadial",
preProc = c("center", "scale"),
tuneLength = 14,
trControl = trainControl(method = "cv"))
##SVM model
svmRTuned <- train(ChemtrainingData$x, ChemtrainingData$y,
method = "svmRadial",
preProc = c("center", "scale"),
tuneLength = 14,
trControl = trainControl(method = "cv"))
#----Evaluation
svmPred <- predict(svmRTuned, newdata = ChemtestData$x)
print(postResample(pred = svmPred, obs = ChemtestData$y))
#KNN Model
knnModel <- train(x = ChemtrainingData$x,
y = ChemtrainingData$y,
method = "knn",
preProc = c("center", "scale"),
tuneLength = 10)
#KNN Model
knnModel <- train(x = ChemtrainingData$x,
y = ChemtrainingData$y,
method = "knn",
preProc = c("center", "scale"),
tuneLength = 10)
#----Evaluation
knnPred <- predict(knnModel, newdata = ChemtestData$x)
print(postResample(pred = knnPred, obs = ChemtestData$y))
### MARS model
marsGrid <- expand.grid(.degree = 1:2,  .nprune = 2:50)
marsFit <- train(x = ChemtrainingData$x,
y = ChemtrainingData$y,
method = "earth",
tuneGrid = marsGrid,
preProc = c("center", "scale"),
trControl = trainControl(method = "cv"))
marsFit <- train(x = ChemtrainingData$x,
y = ChemtrainingData$y,
method = "earth",
tuneGrid = marsGrid,
preProc = c("center", "scale"),
trControl = trainControl(method = "cv"))
#----Evaluation
marsPred <- predict(marsFit, newdata = ChemtestData$x)
print(postResample(pred = marsPred, obs = ChemtestData$y))
print(postResample(pred = marsPred, obs = ChemtestData$y))
### MARS model
marsGrid <- expand.grid(.degree = 1:2,  .nprune = 2:50)
### Firstly the Neural Net Model
netGrid <- expand.grid(.decay = c(0, 0.01, 0.1),.size = 1:10,.bag = FALSE)
nnetTune <- train( ChemtrainingData$x,
ChemtrainingData$y,
method = "avNNet",  tuneGrid = netGrid,
trControl = trainControl(method = "cv"),
preProc = c("center", "scale"),
linout = TRUE,
trace = FALSE,maxit = 500)
nnetTune <- train( ChemtrainingData$x,
ChemtrainingData$y,
method = "avNNet",  tuneGrid = netGrid,
trControl = trainControl(method = "cv"),
preProc = c("center", "scale"),
linout = TRUE,
trace = FALSE,maxit = 500)
#----Evaluation
nnetPred <- predict(nnetTune, newdata = ChemtestData$x)
print(postResample(pred = nnetPred, obs = ChemtestData$y))
svmPred
print(summary(svmPred$bestTune))
print(summary(svmPred$finalModel))
print(svmPred$results)
print(varImp(svmRTuned))
library(ggplot2)
for (var in vars_to_plot) {
print(
ggplot(ChemicalManufacturingProcess, aes_string(x = var, y = "Yield")) +
geom_point() +
#geom_smooth(method = "loess", se = FALSE, color = "blue") +
labs(title = paste("Yield vs", var))
)
}
vars_to_plot <- c("BiologicalMaterial06", "BiologicalMaterial03", "BiologicalMaterial12")
for (var in vars_to_plot) {
print(
ggplot(ChemicalManufacturingProcess, aes_string(x = var, y = "Yield")) +
geom_point() +
#geom_smooth(method = "loess", se = FALSE, color = "blue") +
labs(title = paste("Yield vs", var))
)
}
