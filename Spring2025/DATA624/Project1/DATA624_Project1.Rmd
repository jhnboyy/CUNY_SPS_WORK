---
title: "DATA624_Project1"
author: "John Ferrara"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
library(dplyr)
library(tidyverse)
library(readxl)
library(tsibble)
library(fpp3)
```

# Part A - ATM FORECAST (ATM624Data.xlsx)

## Description
In part A, I want you to forecast how much cash is taken out of 4 different ATM machines for May 2010.  The data is given in a single file.  The variable ‘Cash’ is provided in hundreds of dollars, other than that it is straight forward. I am being somewhat ambiguous on purpose to make this have a little more business feeling.  Explain and demonstrate your process, techniques used and not used, and your actual forecast.  I am giving you data via an excel file, please provide your written report on your findings, visuals, discussion and your R code via an RPubs link along with the actual.rmd file  Also please submit the forecast which you will put in an Excel readable file.

## Overview
There were four different ATMs in the raw dataset. In order to accurately provide forecasts for each individual ATM machine and its respective cash flow the data was parsed so as to separate each of the four ATMs into their own individual datasets. However, before this data was parsed out based on the ATM number several steps were taken to prepare the data for analysis. The dataset was checked for null values. A total of 19 rows out of 1,474 total rows were found to have null values. Of these 19, there were 14 rows that had a null value for the ATM and the Cash value. These rows lacked enough data to be deemed useful towards the analysis and were dropped. The four remaining rows that contained null values were limited to the Cash column. Due to these rows having an ATM value they were left in. The 5 remaining null values impacted ATM1 and ATM2. In order to fill in these values, the average of the Cash values for the following and preceeding days were averaged in order to imputes these values. For instance, one of the null values found that impacted ATM 1 was a null cash value on 6/13/2009. The average of the cash values for ATM1 on 6/12/2009 and 6/14/2009 were averaged to impute the null value on 6/13/2009. This technique was used on each of the 5 total null cash values. 

In addition to imputing null values in the data, the dates, having been sourced from and Excel file, needed formatting. In order to convert the dates from the raw excel format research into Excel's data methodology had to be completed in order to identify the origin date that the program uses to count days. It was discovered that while Excel uses 1900/01/01 as base line date, the program also incorrectly considers 1900 a leap year. This mandates the use of a different origin date for proper date conversions. The date used was '1899-12-30'. Using this date the numbers initially read in were successfully converted to dates to use in the forecast analysis. 

These were the main processing methodologies used on the data. The following sections address the methodology for May 2010 cash withdrawal projections for each of the ATMs. 

### ATM1
The following image is the initial data concerning ATM1 when plotted. As you can see there is no trend but plenty of seasonality on the weekly level. 

![Raw ATM1 Plot](https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA624/Project1/images/atm1_proj_full.png)


After checking for the presence of zeros in the Cash column, as this would impact certain transformation types. I decided to log the data in order to yeild a better performing model. I then proceeded to attempt different modeling techniques to yield an accurate projection. I tried to main modeling methodologies: ETS and ARIMA modeling. There were no zeros in the ATM1 Cash data column, so I manually attempted both additive and multiplicative seasonalities, while also using the function's native ability to identify the best recommended model. The following table breaks down the ETS and ARIMA models I attempted, along with the model I ultimately selected based on performance. 

###### ATM 1 Model Comparison Table (ETS and ARIMA)

| Model           | Model Type | AIC      | AICc     | BIC      | RMSE     | Status |
|-----------------|------------|----------|----------|----------|----------|-------|
| auto_ANA ETA(ANA)      | ETS        | 4488.413 | 4489.035 | 4527.412 | 23.83524 |Rejected |
| ANM             | ETS        | 4488.277 | 4488.898 | 4527.276 | 23.83080 |Rejected |
| MNM             | ETS        | 4566.787 | 4567.408 | 4605.786 | 26.34075 |Rejected|
| MNA             | ETS        | 4595.487 | 4596.108 | 4634.486 | 23.99628 |Rejected|
| manual_select2 ARIMA(0,0,0)(2,1,0) | ARIMA      | 670.4853 | 670.5531 | 682.1269 | 27.30840 |Rejected|
| manual_select3 ARIMA(0,0,0)(3,1,0) | ARIMA      | 661.1413 | 661.2547 | 676.6635 | 26.75715 |Rejected|
| auto_step   ARIMA(0,0,0)(0,1,1)    | ARIMA      | 647.8779 | 647.9117 | 655.6390 | 25.99424 |Selected|
| auto_search ARIMA(0,0,2)(0,1,1)    | ARIMA      | 646.3709 | 646.4842 | 661.8930 | 26.42130 |Rejected|


Using the selected ARIMA model (ARIMA(0,0,0)(0,1,1)), a forecast for ATM1's cash output was made for 30 days after the end of the data, essentially May 2010. This was done after last confirmation of the model's residuals to not have any autocorrelations or odd patterns present by visually looking at the residuals and performing a LJung Box test to reteive a p-value. The projection image can be seen below, with the detailed daily projected values in the accompanying file in the ATM1 tab. 

![Full ATM1 Projection Forecast Plot](https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA624/Project1/images/atm1_proj_full.png)

![Limited for Visibility ATM1 Projection Forecast Plot](https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA624/Project1/images/atm1_proj_lim.png)
Overall, the projection predicts that for the month of may the cash withdrawals will be around $10,000 (as keep in mind the cash column is in hundreds). This does fluctuate a bit, but on 5/1/2010 the predicted value is about 103 for the Cash column, by the 15th of the month the prediction is around 106, and lastly by months end the prediction is ~125. More details can be found in the accompanying Excel file in the PARTA_ATM1 tab. Lastly, all of my code and work for this ATM can be foudn in Appendix A. 


### ATM2
The following image is the initial data concerning ATM2 when plotted. As you can see, similar to ATM1 there is no trend but plenty of seasonality on the weekly level. 

![Raw ATM2 Plot](https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA624/Project1/images/raw_atm2.png)

For this ATM there were 2 zero values in the cash column, which meant without transformation muiltiplicative ETS modeling was not possible. I performed the same process as I did with ATM 1, that is generating several ETS and ARIMA models in order to find one that best fit the data. The model types attempted can be seen in the table below. 

###### ATM 2 Model Comparison Table (ETS and ARIMA)

| Model           | Model Type | AIC      | AICc     | BIC      | RMSE     | Status |
|-----------------|------------|----------|----------|----------|----------|-------|
| auto  ETS(ANA)   | ETS        | 4525.473 | 4526.095 | 4564.472 | 25.07654 |Rejected|
| ANA             | ETS        | 4525.473 | 4526.095 | 4564.472 | 25.07654 |Rejected|
| manual_select2 ARIMA(0,0,0)(2,1,0) | ARIMA      | 3351.374 | 3351.441 | 3363.015 | 25.54244 |Rejected       |
| manual_select3 ARIMA(0,0,0)(3,1,0) | ARIMA      | 3339.141 | 3339.255 | 3354.664 | 25.01099 |Rejected |
| auto_step  ARIMA(2,0,2)(0,1,1)     | ARIMA      | 3318.576 | 3318.816 | 3341.859 | 24.11392 | Selected |
| auto_search ARIMA(2,0,2)(0,1,1)    | ARIMA      | 3318.576 | 3318.816 | 3341.859 | 24.11392 | Selected |

After selecting the best model based on the numbers in the table, which was the ARIMA(2,0,2)(0,1,1) model, the residuals of the selected model were examined for good measure showing no autocorrelation and using Ljung Box tests, the p-values confirmed this as well. I proceeded forward to forecast using this model, which can be seen in the image below. 

![ATM2 Projection Forecast Plot](https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA624/Project1/images/atm2_proj.png)

![Limited for Visibility ATM2 Projection Forecast Plot](https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA624/Project1/images/atm2_proj_lim.png)
Overall, the forecast predicted that the Cash values for the month of May 2010, would average aroudn 6,000, or 60 (remember Cash is valued in hundreds). The forecast outlined that on the first of hte month the predicted values is around 66, by mid-month the forecast remains around 66, and by the end of the month the prediction is 72. There are variations in between. More details can be founf in the accompanying Excel file in the PARTA_ATM2 Tab. Lastly, all of my code and work for this ATM can be foudn in Appendix A. 

### ATM3
The following images are the plots of the raw data of ATM3. The data set was different as all but three of the data points for the Cash column had zero values. The last three points were the only varying data points available. This led to me using a different modeling technique than ATMs 1, 2 and 4.

![Raw ATM3 Plot](https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA624/Project1/images/raw_atm3.png)

![Raw ATM3 Plot Limted for Visualization](https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA624/Project1/images/raw_atm3_lim.png)

For modeling this data set ETS and ARIMA techniques were not used do it only have 3 non-zero data points towards the end of the timeframe covered by the data. Instead, I applied simple forecasting methods: Naive, Mean, and Drift models. After exampling the RMSE numbers, along with other measurements of error for the models I selected the NAIVE modeling method, and used this model to derive projected values for May 2010. 

###### ATM 3 Model Comparison Table (Simple Models)

| Model  | RMSE      | Notes |
|--------|-----------|----------|
| NAIVE  |  5.087423 | Selected |
| MEAN   |  7.933887 | Rejected |
| DRIFT  |  5.082060 | Rejected |

![ATM3 Projection Forecast Plot](https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA624/Project1/images/atm3_proj.png)
Overall, the forecast predicted that the values for May 2010 will be around 85. There is no variation in this model as it was the NAIVE modeling technique that was used due to the lack of nuanced data values. The data seems to show that while the ATM was deployed for a while, it is only recently that it began being used. More details can be found in the accompanying Excel file in the PARTA_ATM3 Tab. Lastly, like all predictions these are estimates, and the confidence intervals were withheld from the final output file for conciseness and clarity, but were included in the visualization. All of my code and work for this ATM can be found in Appendix A. 

### ATM4 

The photo below shows the plotting of the raw ATM4 data. 

![Raw ATM4 Plot](https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA624/Project1/images/raw_atm4.png)

For ATM4, similar to ATM 1 and 2, the data has no clear trend. However, this data also no real pattern to the variations in the data. There was one large outlier that was in the data to note of, I left it because there was a sufficient amount of other data to model with. With this being said, the process for finding a model for ATM 4 was the same process for ATM1 and ATM2. I used ETS and ARIMA methodologies in order to find the best performing model to forecast May 2010 values. 

###### ATM 4 Model Comparison Table (ETS and ARIMA)

| Model           | Model Type | AIC      | AICc     | BIC      | RMSE      | Status |
|-----------------|------------|----------|----------|----------|-----------|-------|
| auto  ETS(M,N,A)          | ETS        | 6690.624 | 6691.246 | 6729.623 | 645.1182  | Rejected |
| ANM             | ETS        | 6884.976 | 6885.598 | 6923.975 | 635.3199  | Rejected |
| MNM             | ETS        | 6795.135 | 6795.756 | 6834.134 | 851.0784  | Rejected |
| MNA             | ETS        | 6690.624 | 6691.246 | 6729.623 | 645.1182  | Rejected |
| manual_select2 ARIMA(0,0,0)(2,1,0) | ARIMA      | 5763.329 | 5763.397 | 5774.971 | 740.9888  | Rejected |
| manual_select3 ARIMA(0,0,0)(3,1,0) | ARIMA    | 5736.843 | 5736.956 | 5752.365 | 710.4476  | Rejected |
| auto_step  ARIMA(0,0,0) w/ mean     | ARIMA      | 5768.064 | 5768.097 | 5775.864 | 650.0437  | Selected |
| auto_search ARIMA(0,0,0) w/ mean    | ARIMA      | 5768.064 | 5768.097 | 5775.864 | 650.0437  | Selected |

After selecting the best model based on the numbers in the table, which was the ARIMA(0,0,0) w/ mean or basically a simple average of the data. The residuals of the selected model were examined for good measure showing odd patterns and using Ljung Box tests, the p-values confirmed no autocorrelation. I proceeded forward to forecast using this model, which can be seen in the image below. 


![ATM4 Projection Forecast Plot](https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA624/Project1/images/atm4_proj.png)

![Limited for Visibility ATM4 Projection Forecast Plot](https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA624/Project1/images/atm4_proj_lim.png)

Overall, the forecast predicted that the Cash value for the month of May in 2010 would be about 475 hundred. There is no variation in this data as it is a mean of the data itself. There is no weekly seasonlity in this data, and seems to just be random fluctuations on withdrawal usage. More details can be found in the accompanying Excel file in the PARTA_ATM4 Tab. Like all predictions these are estimates, and the confidence intervals were withheld from the final output file for conciseness and clarity, but were included in the visualization. Lastly, all of my code and work for this ATM can be found in Appendix A. 


# Part B – FORECASTING POWER (ResidentialCustomerForecastLoad-624.xlsx)

### Description
Part B consists of a simple dataset of residential power usage for January 1998 until December 2013.  Your assignment is to model these data and a monthly forecast for 2014.  The data is given in a single file.  The variable ‘KWH’ is power consumption in Kilowatt hours, the rest is straight forward.  Add this to your existing files above. 

### Overview
For Part B, the raw data was read into R, and processed as needed to prep for analysis. In order to prep the data, several steps were taken. Firstly, when initially read in the date values found in the "YYYY-MMM" column were not formatted as dates. This column was formatted as a date using yearmonth. It was after this that the rest of the data cleaning could take place. The data was checked for null values that were in need of cleaning or imputation. There was one null found in the KWH values. Similar to Part A, because it was just one null compared to the larger data set, this value was filled in using the sandwiching KWH values. The one null was for September 2008, so the values of KWH for October 2008 and August 2008 were averaged to impute the September 2008 null value. After this was completed the data was placed into a tsibble, plotted and various models were applied ot the data. The image below shows the data plotted with the one imputed value. 

![Part B KWH Data Plotted](https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA624/Project1/images/partb_dataplot.png)

Based on the experience in Part A, along with the patterns in the data, a Box-Cox lambda transformation value was used to best transform the data. The box-Cox yielded a value of 0.11, which is close to logging the data, as the most ideal transformation. After this, the ARIMA function was used to find the best fit model. Only ARIMA was used as in Part A ARIMA consistently performed better than the ETS models. The models that the function selected through both a step wise fashion, and the more time intensive method are in the table below. 

### Residential Power Model Comparison Table (ARIMA)

| Model        | Model Type | AIC      | AICc     | BIC      | RMSE      | Notes            |
|--------------|------------|----------|----------|----------|-----------|------------------|
| auto_step  ARIMA(0,1,2)(0,0,2)[12]  | ARIMA      | 619.6074 | 619.9317 | 635.8688 | 1,197,100 |                  |
| auto_search ARIMA(0,1,2)(2,0,0)[12] | ARIMA      | 601.4199 | 601.7443 | 617.6813 | 1,088,172 | Selected model   |


After examining the output data for the models in the table above, the "auto_search" model was selected. I conducted one final round of checks on the model, confirming there was no autocorrelation or patterns in the models' residuals by visually plotting them and performing a Ljung Box test to review the p-value. This model, the ARIMA(0,1,2)(2,0,0)[12], was used to then forecast the KWJ for the next year, or 12 months of 2014. THe forecast visualization can be seen plotted below. 

![PartB KWH Forecast for 2014](https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA624/Project1/images/partb_data_forecast.png)

Overall, the forecast produced by the model outlines that in January 2014 the value for KWH will be ~9,192,398. By mid-year, the value will be the ~7,709,113, and by years end in December the value would be ~7,711,422. As with other ARIMA models there are variations in these values month to month. This means that power demand seems to peak in January 2014, decline until June where increases over the summer. Once fall comes power demand declined again, but slowly increases again in the winter months. Lastly, like all predictions these are estimates, and the confidence intervals were withheld from the final output file for conciseness and clarity, but were included in the visualization. All of my code and work for Part B can be found in Appendix B. 


## APPENDIX A - Part A Code & Analysis
``` {r reading_in_data}
# RAW FILE ALSO SITS HERE: https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/main/Spring2025/DATA624/Project1/ATM624Data.xlsx
#Reading in from local excel 
atm_data <- read_excel("ATM624Data.xlsx")
print(head(atm_data))
print(nrow(atm_data))
## initial Processing notes:
# - Need to handle date, convert to actual date
# - Need to ensure the data is a time series. 

```
### Exploration and Cleaning
``` {r exploration}
## Checking for nulls and other issues.
print(summary(atm_data))

## The cash category has 19 nulls.
## Min of 0 max of 10,919. Mean of 155. 
## Data has no nulls, right now is listed as number. 
## ATM is character, non numeric. Looking at unique vals.

print(unique(atm_data$ATM))
# There is an 'NA' Value here lets take a look

atm_null <- atm_data |> filter(is.na(ATM))

print(nrow(atm_null))
# 14 rows where ATM is null.

print(unique(atm_null$Cash))
# All Cash values are null for where the ATM column is null. These Rows should just be removed? 

## Looking at the additional Cash values that are null with ATM values.
atm_data |> filter(!is.na(ATM), is.na(Cash))
## Cash Nulls with ATM values are limited to ATM 1 and ATM 2. We should impute these 4 rows. 

# Dropping the ATM_Nulls
atm_data <- atm_data |> filter(!is.na(ATM))

```
 

``` {r date_fix_imputations}
## After manually reviwing the data in excel data "DATE" column is actually a time stamp. However, its at midnight for each day so i think a date is good enough. 
## Also had to google how excel does dates for the origin date. Evidently excel incorrectly treats 1900 as a leap year when it wasnt. (INSANE!)
## So need to use 1899
#(Srouce: https://community.alteryx.com/t5/Alteryx-Designer-Desktop-Discussions/Rounding-DateTime-error-when-Importing-Excel/td-p/592023)
atm_data <-atm_data |> mutate(DATE = as.Date(DATE,origin = "1899-12-30"))

# Null Cash Imputation
#atm_data |> filter(is.na(Cash))

## Adding A column to Keep tabs on imputations
atm_data <- atm_data |>mutate(source = if_else(is.na(Cash), "imputed", "original"))

## Firstly Dealing with ATM 1 nulls 
atm_data |> filter(is.na(Cash), ATM =='ATM1')
# The nulls are limited 6/13/2009, 6/16/2009, 06/22/2009

## Checking the data for dates in this month for ATM1
atm_data |> filter(DATE>'2009-06-11',DATE<'2009-06-25' , ATM =='ATM1')

## For ATM 1 there are values sandwiching each of the missing values so were going to just take the average of the "bookend" dates for each missing value. Its only 3 values.
#atm_data |> filter(DATE>'2009-06-11',DATE<'2009-06-15' , ATM =='ATM1') |> mutate()

atm_data_imputed <- atm_data |> mutate(Cash = if_else(is.na(Cash) & ATM == "ATM1" & DATE > as.Date("2009-06-11") & DATE < as.Date("2009-06-15"),
                                  (lag(Cash) + lead(Cash)) / 2, Cash ))
#Confirming
#atm_data_imputed |> filter(DATE>'2009-06-11',DATE<'2009-06-15' , ATM =='ATM1')

## Dealing with 6/16/2009 Null
#atm_data_imputed |> filter(DATE>'2009-06-14',DATE<'2009-06-18' , ATM =='ATM1')
atm_data_imputed <- atm_data_imputed |> mutate(Cash = if_else(is.na(Cash) & ATM == "ATM1" & DATE > as.Date("2009-06-14") & DATE < as.Date("2009-06-18"),
                                  (lag(Cash) + lead(Cash)) / 2, Cash ))
#Confirming
#atm_data_imputed |> filter(DATE>'2009-06-14',DATE<'2009-06-18' , ATM =='ATM1')

# Dealing with the 06/22/2009
atm_data_imputed <- atm_data_imputed |> mutate(Cash = if_else(is.na(Cash) & ATM == "ATM1" & DATE > as.Date("2009-06-20") & DATE < as.Date("2009-06-24"),
                                  (lag(Cash) + lead(Cash)) / 2, Cash ))

#atm_data_imputed |> filter(DATE>'2009-06-20',DATE<'2009-06-24' , ATM =='ATM1')

## FINISHED WITH ATM 1 NULLS, MOVING TO ATM2 NULLS
atm_data_imputed |> filter(is.na(Cash), ATM =='ATM2')
## ATM 2 Nulls are limited to 06/18/2009 and 06/24/2009

## First instance at 6/18/2009
#atm_data_imputed |> filter(DATE>'2009-06-16',DATE<'2009-06-20' , ATM =='ATM2')
atm_data_imputed <- atm_data_imputed |> mutate(Cash = if_else(is.na(Cash) & ATM == "ATM2" & DATE > as.Date("2009-06-16") & DATE < as.Date("2009-06-20"),
                                  (lag(Cash) + lead(Cash)) / 2, Cash ))
#Checking
#atm_data_imputed |> filter(DATE>'2009-06-16',DATE<'2009-06-20' , ATM =='ATM2')

# Second instance at 6/24/2009
#atm_data_imputed |> filter(DATE>'2009-06-22',DATE<'2009-06-26' , ATM =='ATM2')
atm_data_imputed <- atm_data_imputed |> mutate(Cash = if_else(is.na(Cash) & ATM == "ATM2" & DATE > as.Date("2009-06-22") & DATE < as.Date("2009-06-26"),
                                  (lag(Cash) + lead(Cash)) / 2, Cash ))
#Checking
#atm_data_imputed |> filter(DATE>'2009-06-22',DATE<'2009-06-26' , ATM =='ATM2')

## Double checking there are no more nulls
#atm_data_imputed |> filter(is.na(Cash))

## Converting DF to a tsibble with no nulls
atm_tsibble <- atm_data_imputed |> as_tsibble(index = DATE, key = ATM)
#autoplot(atm_tsibble)
```

### Preliminary Analysis
``` {r prelim_analysis}
## Our goal is to parse out the cash (in hundreds) from each atm and give a projection for May 2010. Lets start to look at each atm. 

## ATM1 
atm1 <- atm_tsibble |> filter(ATM =='ATM1')
print(autoplot(atm1))
#p <- autoplot(atm1)
#ggsave("images/raw_atm1.png", plot = p, width = 12, height = 8, dpi = 300)

## Notes: Definitely seasonality here, but seemingly on a weekly or monthly time frame. Other than that variation there isnt really a trend, the data is fairly flat, 

## ATM2 
atm2 <- atm_tsibble |> filter(ATM =='ATM2')
print(autoplot(atm2))
#p <- autoplot(atm2)
#ggsave("images/raw_atm2.png", plot = p, width = 12, height = 8, dpi = 300)
## Notes: Similar to ATM 1, no real trend, but there is definitely seasonality on a weekly or monthly timeframe. 

## ATM 3
atm3 <- atm_tsibble |> filter(ATM =='ATM3')
print(autoplot(atm3))
#p <- autoplot(atm3)
#ggsave("images/raw_atm3.png", plot = p, width = 12, height = 8, dpi = 300)
## NOTES: ATM 3 has 0 withdrawls for the vast majority of the timeframe looked at in the data. Need to chop this chart down a bit to see the actual values. 
## Data doesnt start until 4/28, in order to predict this ATM maybe we can use1,2, and 4? 
lim_atm3 <- atm3 |> filter(DATE>=as.Date('2010-04-26'))
print(autoplot(lim_atm3))
#p <- autoplot(atm3 |>  filter(DATE>=as.Date('2010-04-26')))
#ggsave("images/raw_atm3_lim.png", plot = p, width = 12, height = 8, dpi = 300)

## ATM 4 
atm4 <- atm_tsibble |> filter(ATM =='ATM4')
print(autoplot(atm4))
## notes: Plenty of data, there is one or 2 data points that are severe outliers and will influence any forecast. Other than that there doesnt seem to be any patterns to the data. Also there is no trend in the data. 
#p <- autoplot(atm4)
#ggsave("images/raw_atm4.png", plot = p, width = 12, height = 8, dpi = 300)

## Overall Take aways:
## - ATM 1, 2, 4 have no trend, but have seasonaility
## - ATM 3 has trend, but also limited to 3 data points. may need to estimate using other atms. 

```
### ATM1 Work
``` {r atm1}
#atm1
#autoplot(atm1)

#Chcking for zeros
print(atm1 |> filter(Cash==0)) # No zeros, good for ETS

## Looking at ETS
auto_ets_atm1 <- atm1 |> model(auto_ANA = ETS(Cash), #ETS(A,N,A) is retrieved as the best automatic fit. 
                               ANM = ETS(Cash ~ error("A") + trend("N") + season("M")),
                               MNM = ETS(Cash ~ error("M") + trend("N") + season("M")),
                               MNA = ETS(Cash ~ error("M") + trend("N") + season("A")))


print(auto_ets_atm1 |> report())
## Two best ETS models are ANM and the AUTO, the ANM is nearly the same, but a little better with the AIC, AICc, and BIC models.

print(auto_ets_atm1 |> accuracy())
## The RMSE is basically the same for both, i think i may go with the ANM model as opposed to the ANA auto model. 

## Rerunning with just the good models.
auto_ets_atm1 <- atm1 |> model(auto_ANA = ETS(Cash), #ETS(A,N,A) is retrieved as the best automatic fit. 
                               ANM = ETS(Cash ~ error("A") + trend("N") + season("M")))


##   ------------------------------- Looking at ARIMA Models  -------------------------------

## Checking for Stationarity before modeling. No trend so may not need altering, or at least a small amoutn of altering.

## Need to check stationarity 
print(gg_tsdisplay(atm1,plot_type ="partial")) 
# Could probably use a transformation. No zeros so just as log, no +1 needed.


print(atm1 |> gg_tsdisplay(difference(log(Cash)),plot_type ="partial")) 
# Looks much more stationary with log and one seasonal differencing at 7 for weekly seasonality. 
# ACF Shows some autocorrelation at week-level increments.
# PACF Shows similar. At increments of 6. So this may be AR if 2 or 3. No non-seasonal outlier. ARIMA(0,0,0)(2,1,0)[7] ?

#
print(atm1 |> gg_tsdisplay(difference(log(Cash),7),plot_type ="partial"))
## SUing the PACF side more, there are no non seasonal oputliers, the seasonal outliers are 7,14,21, so 2 or 3 for seasonal. 

arima_atm1 <- atm1 |> 
  model(manual_select2 = ARIMA(log(Cash) ~ pdq(0,0,0) + PDQ(2,1,0)),
        manual_select3 = ARIMA(log(Cash) ~ pdq(0,0,0) + PDQ(3,1,0)),
        auto_step = ARIMA(log(Cash)), #<ARIMA(0,0,0)(0,1,1)[7]>	
        auto_search = ARIMA(log(Cash), stepwise = FALSE, approx=FALSE) ) 
arima_atm1
print(arima_atm1 |> report())
## Manually selected models are not as good as automated ones according to AIC, AICc and BIC. I think auto_step model is the best, will be using that to forecast. 

print(arima_atm1 |> accuracy())
## Confirming that auto_step is the best model. RMSE is 25.99


## COMPARING ETS AND ARIMA
### AUTOSTEP ARIMA (<ARIMA(0,0,0)(0,1,1)[7]>) -> 25.99424, AIC = 647.8779, AICc=	647.9117 , BIC=	655.6390
### ANM ETS -> RMSE=23.83080, AIC = 4488.277	, AICc= 4488.898, BIC=	4527.276	
  

## OVERALL the RMSE is slightly better on the ANM model, but the AIC,AICc and BIC values for the ARIMA are much better. Will be using ARIMA to forecast. 
## Running again with only selected model 
arima_atm1 <- atm1 |> 
  model(auto_step = ARIMA(log(Cash))) 

arima_atm1
## Last levels of confirmation checks
## looking at residuals
print(gg_tsresiduals(arima_atm1))

# Ljung Box test 
print(augment(arima_atm1) |> features(.innov, ljung_box, lag=7, dof=1)) # Lag of 7 gets a pval of 0.07, above 0.05 
## Tryign with mulitples of 7
print(augment(arima_atm1) |> features(.innov, ljung_box, lag=14, dof=1)) #pval of 0.39
print(augment(arima_atm1) |> features(.innov, ljung_box, lag=21, dof=1)) #pval of 0.73

# NO autocorrelation left in the residuals so its good. Movbing forward with forecast
arima_atm1 <- atm1 |> 
  model(auto_step = ARIMA(log(Cash)))

### With model selected taking a look at the residuals for the 
atm1_forecast <- arima_atm1 |> 
  forecast(h = 30) #30 days

# ATM 1 forecast  
print(atm1_forecast |> 
    autoplot(atm1|>filter(DATE>=as.Date('2010-01-01'))))
#p <- atm1_forecast |>     autoplot(atm1|>filter(DATE>=as.Date('2010-01-01')))
#ggsave("images/atm1_proj_lim.png", plot = p, width = 12, height = 8, dpi = 300)

print(atm1_forecast |> autoplot(atm1))
#p <- atm1_forecast |>     autoplot(atm1)
#ggsave("images/atm1_proj_full.png", plot = p, width = 12, height = 8, dpi = 300)



## Looking at forecasted values 
#atm1_forecast|> hilo() |> as_tsibble()

#forecast_table <- atm1_forecast |> hilo() |> as_tsibble()

# Write to CSV
#write.csv(atm1_forecast, "projection_data/atm1_forecast_values.csv", row.names = FALSE)


```
### ATM2 Work
```{r atm2}
#Chcking for zeros
print(atm2 |> filter(Cash==0)) # Two zero values, would need to consider 

autoplot(atm2)

## Looking at ETS
auto_ets_atm2 <- atm2 |> model(auto = ETS(Cash), 
                               ANA = ETS(Cash ~ error("A") + trend("N") + season("A")) #only additive because of 0 values and no trend
                               )

print(auto_ets_atm2) #ETS(ANA) is retrieved as the best automatic fit. 
## ANA is best ETS fit. 

## Rerunning with just the good models.
auto_ets_atm2 <- atm2 |> model(auto_ANA = ETS(Cash))

print(auto_ets_atm2 |> report())
# AIC =4525.473, AICc= 4526.095, BIC=4564.472 

print(auto_ets_atm2 |> accuracy())
#RMSE 25.07654	


##   ------------------------------- Looking at ARIMA Models  -------------------------------

## Checking for Stationarity before modeling. No trend so may not need altering, or at least a small amount of altering.

print(gg_tsdisplay(atm2,plot_type ="partial")) 
# Mainly stationary, could probably use a transformation. Two zeros so log +1 woulb e needed needed.


print(atm2 |> gg_tsdisplay(difference(Cash,7),plot_type ="partial")) 
# Looks much more stationary with one seasonal differencing at 7 for weekly seasonality. 
# ACF Shows some one outlier at 7
# PACF shows outliers  at 7 14 and 21. Biggest at 7. No non-seasonal outlier. ?


arima_atm2 <- atm2 |> 
  model(manual_select2 = ARIMA(Cash ~ pdq(0,0,0) + PDQ(2,1,0)),
        manual_select3 = ARIMA(Cash ~ pdq(0,0,0) + PDQ(3,1,0)),
        auto_step = ARIMA(Cash), # SELECTED: <ARIMA(2,0,2)(0,1,1)[7]>

        auto_search = ARIMA(Cash, stepwise = FALSE, approx=FALSE) )# ALSO SELECTED <ARIMA(2,0,2)(0,1,1)[7]>
 

#print(arima_atm2)
print(arima_atm2 |> report())
## Manually selected models are not as good as automated ones according to AIC, AICc and BIC. I think auto models are the same, and better.
## <ARIMA(2,0,2)(0,1,1)[7]> AIC = 3318.576, AICc=3318.816, BIC=3341.859

print(arima_atm2 |> accuracy())
## Confirming that auto_step is the best model. RMSE is 24.11


## ---- COMPARING ETS AND ARIMA ----
### AUTOSTEP ARIMA -> 24.11, AIC = 3318.576, AICc=3318.816, BIC=3341.859
### ANA ETS -> RMSE=25.07654, AIC =4525.473, AICc= 4526.095, BIC=4564.472 	
  

## OVERALL the ARIMA numbers are much better. Will be using ARIMA to forecast. 

## Running again with only selected model 
arima_atm2 <- atm2 |> 
  model(auto_step = ARIMA(Cash), # SELECTED: <ARIMA(2,0,2)(0,1,1)[7]>
        )


## Last levels of confirmation checks
## looking at residuals
print(gg_tsresiduals(arima_atm2))

# Ljung Box test 
print(augment(arima_atm2) |> features(.innov, ljung_box, lag=7, dof=5)) # pval .031
## Tryign with mulitples of 7
print(augment(arima_atm2) |> features(.innov, ljung_box, lag=14, dof=5)) #pval of 0.34
print(augment(arima_atm2) |> features(.innov, ljung_box, lag=21, dof=5)) #pval of 0.40

# NO autocorrelation left in the residuals so its good. Moving forward with forecast
arima_atm2 <- atm2 |> 
  model(auto_step = ARIMA(Cash))

### With model selected taking a look at the residuals for the 
atm2_forecast <- arima_atm2 |> 
  forecast(h = 30) #30 days

# ATM 2 forecast  
print(atm2_forecast |> 
    autoplot(atm2))
#p <- atm2_forecast |> autoplot(atm2)
#ggsave("images/atm2_proj.png", plot = p, width = 12, height = 8, dpi = 300)

print(atm2_forecast |> 
    autoplot(atm2|>filter(DATE>=as.Date('2010-01-01'))))
#p <- atm2_forecast |> autoplot(atm2|>filter(DATE>=as.Date('2010-01-01')))
#ggsave("images/atm2_proj_lim.png", plot = p, width = 12, height = 8, dpi = 300)

## Looking at forecasted values 
#atm2_forecast|> hilo() |> as_tsibble()

#forecast_table <- atm2_forecast |> hilo() |> as_tsibble()

# Write to CSV
#write.csv(atm2_forecast, "projection_data/atm2_forecast.csv", row.names = FALSE)

```

### ATM3 Work

``` {r atm3}
#Chcking for zeros
print(atm3 |> filter(Cash==0)) # 362 Zero values. this ATm Projection will need to be different from other 3
print(atm3 |> filter(Cash!=0)) ## Only Three data points to use here. Different from the other three projections.

autoplot(atm3)

## Simple Modeling options Mean, NAIVe, and DRIFT 
simple_models <- atm3 |> model(NAIVE = NAIVE(Cash),
              MEAN = MEAN(Cash),
              DRIFT = RW(Cash~drift()))

print(simple_models |> report())

print(simple_models |> accuracy())
# The error measures seem tobe the lowest fro the NAIVE model, so I will use this for the forecast.

## Doing again with just NAIVE
simple_models <- atm3 |> model(NAIVE = NAIVE(Cash))

### With model selected taking a look at the residuals for the 
atm3_forecast <- simple_models |> forecast(h = 30) #30 days

# ATM 3 forecast  
print(atm3_forecast |> 
    autoplot(atm3))
#p <- atm3_forecast |> autoplot(atm3)
#ggsave("images/atm3_proj.png", plot = p, width = 12, height = 8, dpi = 300)

## Looking at forecasted values 
#atm3_forecast#|> hilo() |> as_tsibble()

#write.csv(atm3_forecast, "projection_data/atm3_forecast_values.csv", row.names = FALSE)

```

### ATM 4 Work
``` {r atm4}
#Chcking for zeros
print(atm4 |> filter(Cash==0)) # No Zeros 

autoplot(atm4) # one large outlier.

## Looking at ETS
auto_ets_atm4 <- atm4 |> model(auto = ETS(Cash), #ETS(M,N,A) is retrieved as the best automatic fit. 
                               ANM = ETS(Cash ~ error("A") + trend("N") + season("M")),
                               MNM = ETS(Cash ~ error("M") + trend("N") + season("M")),
                               MNA = ETS(Cash ~ error("M") + trend("N") + season("A")))

print(auto_ets_atm4) 
## ANA is best ETS fit. 



print(auto_ets_atm4 |> report()) # Best model ETS(M,N,A)
# AIC =6690.624, AICc= 6691.246, BIC=6729.623 		

print(auto_ets_atm4 |> accuracy())
#RMSE 645.1182, not lowest but when using above indicotrs too Going with the auto selected model. 	

## Rerunning with just the good models.
auto_ets_atm2 <- atm4 |> model(auto = ETS(Cash))


##   ------------------------------- Looking at ARIMA Models  -------------------------------

## Checking for Stationarity before modeling. No trend so may not need altering, or at least a small amount of altering.

print(gg_tsdisplay(atm4,plot_type ="partial")) 
# Mainly stationary already. No transformations may be needed, but will check anyway. 


print(atm4 |> gg_tsdisplay(difference(Cash,7),plot_type ="partial")) 
# The 7 day seasonal difference looks much better, so doing that. 
# ACF Shows some one outlier at 7
# PACF shows outliers  at 7 14 and 21. Biggest at 7. No non-seasonal outlier.


arima_atm4 <- atm4 |> 
  model(manual_select2 = ARIMA(Cash ~ pdq(0,0,0) + PDQ(2,1,0)),
        manual_select3 = ARIMA(Cash ~ pdq(0,0,0) + PDQ(3,1,0)),
        auto_step = ARIMA(Cash), # SELECTED: <ARIMA(0,0,0) w/ mean>
        auto_search = ARIMA(Cash, stepwise = FALSE, approx=FALSE) )# ALSO SELECTED <<ARIMA(0,0,0) w/ mean>>
 
## Auto Step and Auto search give same model: <ARIMA(0,0,0) w/ mean>
#print(arima_atm4)
# So best forecast is just the mean of the data. 

print(arima_atm4 |> report())
## Manually selected models are not as good as automated ones according to AIC, AICc and BIC. I think auto models are the same, and better.
## <<ARIMA(0,0,0) w/ mean> AIC = 5768.064, AICc=5768.097, BIC=5775.864 	

print(arima_atm4 |> accuracy())
## Confirming that auto_step is the best model. RMSE is 650.0437


## ---- COMPARING ETS AND ARIMA ----
### AUTOSTEP ARIMA -> RMSE = 650.0437, AIC = 5768.064, AICc=5768.097, BIC=5775.864 
### MNA ETS -> RMSE=645.1182, AIC =6690.624, AICc= 6691.246, BIC=6729.623 		
  

## OVERALL the ARIMA numbers are much better. Will be using ARIMA to forecast. 

## Running again with only selected model 
arima_atm4 <- atm4 |> 
  model(auto_step = ARIMA(Cash), # SELECTED: ARIMA(0,0,0) w/ mean>>
        )


## Last levels of confirmation checks
## looking at residuals
print(gg_tsresiduals(arima_atm4))

# Ljung Box test 
print(augment(arima_atm4) |> features(.innov, ljung_box, lag=7, dof=0)) # pval .92

# NO autocorrelation left in the residuals so its good. Moving forward with forecast


### With model selected taking a look at the residuals for the 
atm4_forecast <- arima_atm4 |> 
  forecast(h = 30) #30 days

# ATM 4 forecast  
print(atm4_forecast |> 
    autoplot(atm4|>filter(DATE>=as.Date('2010-01-01'))))
#p <- atm4_forecast |> autoplot(atm4|>filter(DATE>=as.Date('2010-01-01')))
#ggsave("images/atm4_proj_lim.png", plot = p, width = 12, height = 8, dpi = 300)

print(atm4_forecast |> 
    autoplot(atm4))
#p <- atm4_forecast |> autoplot(atm4)
#ggsave("images/atm4_proj.png", plot = p, width = 12, height = 8, dpi = 300)


## Looking at forecasted values 
#atm4_forecast|> hilo() |> as_tsibble()

#forecast_table <- atm4_forecast |> hilo() |> as_tsibble()

# Write to CSV
#write.csv(atm4_forecast, "projection_data/atm4_forecast_values.csv", row.names = FALSE)


```


## APPENDIX B - Part B Code & Analysis
``` {r reading_data}
# RAW FILE ALSO SITS HERE: https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/main/Spring2025/DATA624/Project1/ResidentialCustomerForecastLoad-624.xlsx
res_df<-read_excel('ResidentialCustomerForecastLoad-624.xlsx')
#res_df
```

``` {r cleaning_exploring}

summary(res_df)
## There is one null in KWH
## No Nulls in CaseSequence

res_df |> filter(is.na('YYYY-MMM'))
## No null in dates

## Looking at the one null 
res_df|> filter(is.na(KWH))
## The one null in KWH is in September 2008, checking the before and after

res_df<- res_df |>
    mutate(DATE = yearmonth(`YYYY-MMM`),
           source = if_else(is.na(KWH), "imputed", "original"))

#res_df |> filter( DATE >= yearmonth('2008-Aug') & DATE<=yearmonth('2008-Oct'))

red_df_imp <- res_df |> mutate(KWH = if_else(is.na(KWH) & DATE >= yearmonth('2008-Aug') & DATE<=yearmonth('2008-Oct'),
                                  (lag(KWH) + lead(KWH)) / 2, KWH ))
## Checking imp
#red_df_imp |> filter(DATE>=yearmonth('2008-Aug'),DATE<=yearmonth('2008-Oct'))

## Placing into a Tsible with the Case Sequence removed
red_tsibble <- red_df_imp |>
  as_tsibble(index = 'DATE', key = CaseSequence)|>
  summarize(Total_KWH = sum(KWH))

# plotting
print(red_tsibble|> autoplot())
#p <- red_tsibble|> autoplot()
#ggsave("images/partb_dataplot.png", plot = p, width = 12, height = 8, dpi = 300)
## Very subtle trend with seasonal variation. One large drop / outlier. 

## Using Box -Cox for best transformation 
lambda <- red_tsibble |>
  features(Total_KWH, features = guerrero) |>
  pull(lambda_guerrero)
print(lambda) ## 0.107 Close to a log transformation lambda

## Transformed data looks good
print(red_tsibble |>  autoplot(box_cox(Total_KWH, lambda)) +  labs(y = "",title = latex2exp::TeX(paste0("BoxCox Transformed Total KWH ",round(lambda,2)))))

## Jumping right into ARIMA models because ARIMA was the better model for nearly all of part 1. 
## Similiarly only doing the automated best model find, as in part 1 nearly all the time the function was correct. 


red_model <- red_tsibble |> 
  model(auto_step = ARIMA(box_cox(Total_KWH, lambda)), # SELECTED: <ARIMA(0,1,2)(0,0,2)[12]>	
        auto_search = ARIMA(box_cox(Total_KWH, lambda), stepwise = FALSE, approx=FALSE) )# Selected:<ARIMA(0,1,2)(2,0,0)[12]>	
## Seeing Selections
print(red_model)

## Selected to slightly different models, now going to compare both for best one. 
print(red_model|> report())
#auto_search (<ARIMA(0,1,2)(2,0,0)[12]>) is better: AIC:601.4199,	AICc:601.7443, 	BIC:617.6813	
print(red_model|> accuracy())
## for error measures the auto_search is also beterr with lower error values accross the table. 

## Redoing the model with just the better one in preparation for forecasting. 
red_model <- red_tsibble |> 
  model(auto = ARIMA(box_cox(Total_KWH, lambda), stepwise = FALSE, approx=FALSE) )# Selected:<ARIMA(0,1,2)(2,0,0)[12]>	

## Last levels of confirmation checks
## looking at residuals
print(gg_tsresiduals(red_model)) # Fine with an outlier or 2

# Ljung Box test 
print(augment(red_model) |> features(.innov, ljung_box, lag=12, dof=4)) # pval .35
print(augment(red_model) |> features(.innov, ljung_box, lag=24, dof=4)) # pval .45

# NO autocorrelation left in the residuals so its good. Moving forward with forecast


### With model selected taking a look at the residuals for the 
red_forecast <- red_model |> forecast(h = 12) #12 months

# KWH forecast  
print(red_forecast |> 
    autoplot(red_tsibble))
#p <- red_forecast |> 
    autoplot(red_tsibble)
#ggsave("images/partb_data_forecast.png", plot = p, width = 12, height = 8, dpi = 300)

## Looking at forecasted values 
#red_forecast|> hilo() |> as_tsibble()

#forecast_table <- red_forecast |> hilo() |> as_tsibble()

#write.csv(red_forecast, "projection_data/red_forecast_values.csv", row.names = FALSE)

```
 
