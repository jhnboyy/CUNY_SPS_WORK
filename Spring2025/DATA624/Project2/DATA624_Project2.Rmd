---
title: "DATA624_Project2"
author: "John Ferrara, Javier Pajuelo Bazan, Benson Yikseong Toi, Bikash Bhowmik, Jose Fuentes"
date: "2025-04-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("kernlab")
library(readxl)
library(httr)
library(dplyr)
library(mice)
library(pheatmap)
library(GGally)
library(ggplot2)
library(reshape2) 
library(corrplot)
library(caret)
library(earth)
library(kernlab)
library(randomForest)
library(rpart)
library(rpart.plot)

#library(moments)
#library(tidyr)
library(tidyverse)
#library(MASS)

```

## Project #2 (Team) Assignment

This is role playing.  I am your new boss.  I am in charge of production at ABC Beverage and you are a team of data scientists reporting to me.  My leadership has told me that new regulations are requiring us to understand our manufacturing process, the predictive factors and be able to report to them our predictive model of PH.

Please use the historical data set I am providing.  Build and report the factors in BOTH a technical and non-technical report.  I like to use Word and Excel.  Please provide your non-technical report in a  business friendly readable document and your predictions in an Excel readable format.   The technical report should show clearly the models you tested and how you selected your final approach.

Please submit both Rpubs links and .rmd files or other readable formats for technical and non-technical reports.  Also submit the excel file showing the prediction of your models for pH.

## Objective and Data Overview 

The goal of this project was to use the data provided via Excel format in order to help model which predictor variables were ideal for keeping track of and predicting the PH variable for the hypothetical company ABC Beverage. This analysis was conducted in the context of a hypothetical new regulatory requirement, which mandates a better understanding and control of key production metrics, including beverage acidity (PH).

There were two datasets provided. The first, used for training and modeling, contained 2,571 observations and 33 variables, including the target variable PH. This dataset served as the foundation for all of the exploratory analysis, data cleaning, and model development. The second dataset, intended for implementing the yielded predictions, included 267 observations with all of the same predictor variables. However, this dataset lacked values for PH.

The datasets covered a wide range of operational metrics from the beverage production process, with variables representing temperatures, pressures, flow rates, brand identifiers, and more. The ultimate goal was to generate a predictive model that could estimate PH based on these variables, supporting both compliance needs and improved operational oversight.

### Reading In the Data
```{r}
## Reading in the data 
training <- read_excel("./data/StudentData.xlsx")
test <- read_excel("./data/StudentEvaluation.xlsx")

## Confirming each was successfully read in.
print(head(training))
print(head(test))

```
## Data Cleaning and Preparation

Exploratory analysis began with the creation of summary statistics, correlation matrices, and a series of pairwise plots using GGally. All of these methods were leveraged to get a better sense of the data, and the relationships between variables. The correlation matrix revealed several strong linear relationships among the variables. For instance, Carb Temp and Carb Pressure, as well as Density and Balling, were found to be highly correlated. This indicates potential multicollinearity within the dataset. However, no corrective steps were taken due to the intended use of modeling techniques such as PLS manage these types of relationships between predictors. While some variables showed skewed or mnon-normal distributions, this was not considered problematic, especially for due to the choice of modeling approaches. 

As part of the data preparation, there were null values in the data that needed to be handled. Firstly, any rows in the training set where the target variable PH was missing were removed, as these rows could not contribute to model training. Additionally, rows with four or more missing values were dropped from both the training and testing datasets to avoid introducing too much uncertainty into the imputation process. For the remaining missing values, a multiple imputation strategy was implemented using MICE. The numeric variables were imputed using Predictive Mean Matching and the singular categorical variable, Brand Code, was imputed separately using a Classification and Regression Tree method. This process resulted in fully imputed datasets for both training and test sets, enabling a complete and consistent modeling pipeline.

Lastly, in order to obtain solid results, the training data was split 80/20 for triaing testing. As mentioned before, the prediction testing dataset received, had no PH values. THerefore in order to test multiple models, a smaller test set was needed to help select a model. Data was also centered and scaled prior to modeling, but within the modeling functions, to account for differing measurement units.

### Exploratory Work and Data Processing

```{r , fig.width=14, fig.height=12}
## Beginning to look at the training data
print(summary(training))
## All column have some null values in them, so imputation will be needed. the two columns that dont have null values are "Pressure Vacuum" and "Air PRessurer" 
## upon first glance for the manufacutring process, there is no immediate way of identifying what will be important. Going to do GGpairs plot in order to view the variables, The instructions state that "PH" is the target variable.

## The columns with the highest number of nulls are as follows: 
## - MFR with 212 nulls.
## - Filler Speed with 57 nulls.
## - PC Volume 39 nulls
## - PSC CO2 39 nulls
## - Fill Ounces 38 nulls 
## - Carb Pressure1 32 nulls
## - PSC 33 nulls
## - Hyd Pressure4 30 nulls
## All of the rest of the columns have less than 30 rows with nulls. 

print(nrow(training)) #2,571 rows of data in the training. 

print(length(names(training))) ## 33 different columns with PH as target, so 32 predictor variables. 

## Overall this means that most of the columns have 1% or less of null values. Next step would be to look and see if the null values are together. In other words do 20 rows of incomplete data accoutn for most of the predictor variable nulls? 
```


##### Histograms

Next, we show a histogram overview of all features in the train_data and its corresponding skewness.
```{r hist, warning=FALSE, message=FALSE, echo=FALSE, fig.height=25, fig.width=20}
# high level view of histgrams without nans

# no need to include INDEX
hist_data <- train_data %>% select(-c(`Brand Code`))


gather_features <- hist_data %>% 
  gather(key = 'features', value = 'value')

ggplot(gather_features) + 
geom_histogram(aes(x=value, y =..density..), bins=30) + 
geom_density(aes(x=value), color='green') +
facet_wrap(.~features, scales='free', ncol=4) +
theme(strip.text = element_text(size=13))
```

*Normal Distribution.-* Carb Pressure, Carb Temp, Fill Ounces, PC Volume, PH

*Right Skewed.-* Air Pressurer, Hyd Pressure1, Hyd Pressure2, Hyd Pressure3, Mnf Flow,  Oxygen Filler, PSC, PSC CO2, PSC Fill, Temperature.

*Left Skewed.-* Filler Speed, MFR, Usage cont

*Bi-Modal.-* Alch Rel, Balling, Balling Lvl, Carb Flow, Carb Pressure1, Carb Rel, Carb Volume, Density, Fill Pressure, Hyd Pressure4.


#### Cleaning Training Data

```{r, }

## Firstly, i think we can drop those rows where the target variable PH is null, THis is what we are trying to figure out. So if that variable is null in training this is not worth keeping. 
training_nonullPH <- training %>%  filter(!is.na(PH))

print(head(training_nonullPH))
print(nrow(training_nonullPH)) #2,567 only a difference of 4 rows.

na_matrix <- is.na(training_nonullPH) * 1
na_matrix_t <- t(na_matrix) 

## Taking a look at a prelim heatmap for where nulls all exist.
pheatmap(na_matrix_t,
         cluster_rows = FALSE,
         cluster_cols = FALSE,
         color = c("white", "red"),
         fontsize_row = 8, 
         fontsize_col = 8)  

### Looking at how many rows have over X number of nulls in the row. 
missing_per_row <- rowSums(is.na(training_nonullPH))
missing_summary <- as.data.frame(table(missing_per_row))
colnames(missing_summary) <- c("Missing_Count", "Row_Count")
print(missing_summary)

## Looking at the break down for the multiple nulls. If there are 4 or more columns with a null value in a row, going to drop. We can imput the others, as the number of rows with <=3 null values increases fast.

## Executing the drop of those rows here, using the row index?
training_cleaned <- training_nonullPH[missing_per_row < 4, ]

print(nrow(training_cleaned)) #2547, dropped 20 rows from the data. Will impute the rest of the data. 

# Confirming data types before moving forward, everything should be numeric (except a few excpetions)
print(str(training_cleaned))

### Question for Brand Code, do we want to imput this column or just drop because categorical. Maybe tree? 
training_cleaned |> filter(is.na(training_cleaned$'Brand Code')) ### `115 rows, dont want to drop. so should impute. But differently from the numbers.

## Need to make chartacter BrandCode column into a categorizcal var instract of stirng / character
training_cleaned$`Brand Code` <- as.factor(training_cleaned$`Brand Code`)

#checking
str(training_cleaned$`Brand Code`)


```

#### Imputing Training Data

```{r, message=FALSE, warning=FALSE }
print(summary(training_cleaned))
## Columns with no nulls, making note because they may be empty in the method df
## Mnf Flow, Density, Balling Lvl, RowID, PH, Air Pressurer, Balling, Pressure Vacuum

# Imputing the data with MICE, using a different imputation form for the Brand Code Column 
## Getting the method df first, to confirm methods.
init <- mice(training_cleaned, maxit = 0)
meth <- init$method  

print(meth)
meth$'Brand Code' <- "cart" # We want the categorical tree for this variable, not  a continuous numeric var. 
print(meth)

imputed_data <- mice(training_cleaned, method = meth, m = 10, seed = 100)
summary(imputed_data)

## Taking one fo the imputed datasets for now, if i want to use all of them alter i will. 
working_data <- complete(imputed_data, 1)

### Successfully got rid of all nulls. 
summary(working_data)

## Confirming it worked, specifically for brand code. 
unique(working_data$'Brand Code')
print(working_data |> filter(is.na(working_data$'Brand Code')))

## Lastly, dropping Row ID because irrelevant for the model.
working_data <- working_data %>% select(-`RowID`)

```


##### Scatter Plots Train Data
Next we show the relationship between PH and every single other feature where every feature is on the x-axis and PH on the y-axis.
There are many outliers in many predictors.

There are many linear relationships between PH and several predictors where the relationship is a straight horizontal line, meaning the slope = 0 and y=b where b is "semi-constant".

Other predictors show a quadratic relationship with PH.

```{r,warning=FALSE, message=FALSE, echo=FALSE,  fig.height=25, fig.width=20}

# numeric only 
out<- working_data %>%select(-c("Brand Code"))
theme1 <- trellis.par.get()
theme1$plot.symbol$col = rgb(.2, .2, .2, .4)
theme1$plot.symbol$pch = 16
theme1$plot.line$col = rgb(1, 0, 0, .7)
theme1$plot.line$lwd <- 2
theme1$fontsize$text <- 7

trellis.par.set(theme1)
caret::featurePlot(x=out[,2:ncol(out)], 
                   y=out[[1]], 
                   type = c("p", "smooth"),
                   
                   span = .5)
``` 

#### Correlations
Next we show correlations greater than 0 percent among features.
TODO  confirm data is highly uncorrelated...

```{r high_correlations, fig.width=50, fig.height=50}
#, echo=FALSE}

# foundd large correlations by doing this
# mydf <- train_no_nans
#mydf <- train_data %>% select(-c(`Brand Code`))

mydf <- working_data %>%select(-c("Brand Code"))


d2 <- mydf %>% 
     as.matrix %>%
     cor %>%
     as.data.frame %>%
    rownames_to_column(var = 'var1') %>%
     gather(var2, value, -var1)

d3<-d2 %>%
     mutate(var_order = paste(var1, var2) %>%
                strsplit(split = ' ') %>%
                map_chr( ~ sort(.x) %>% 
                             paste(collapse = ' '))) %>%
     mutate(cnt = 1) %>%
     group_by(var_order) %>%
     mutate(cumsum = cumsum(cnt)) %>%
    filter(cumsum != 2) %>%
  ungroup %>%
  dplyr::select(-var_order, -cnt, -cumsum)

tmp <- d3 %>%filter(value>.75 & var1!=var2)

knitr::kable(tmp %>% arrange(desc(value)))

```

```{r, fig.width=45, fig.height=45}

corrplot(cor(working_data %>%select(-c("Brand Code")))
         , method = 'number'
        , tl.cex=3
        , cl.cex=3
        , number.cex=3
         )

```

PH itself is not highly correlated to any of its predictors. However, there are many predictors highly correlated among themselves.

All highly predictors with over 75 percent correlation:
Hyd Pressure1 & Hyd Pressure2 – 0.92
Hyd Pressure1 & Hyd Pressure3 – 0.90
Hyd Pressure2 & Hyd Pressure3 – 0.92
Filler Speed & MFR – 0.94
Carb Flow & Density – 0.95
Balling & Density – 0.90
Balling & Carb Flow – 0.95
Balling & MFR – 0.93
Balling & Balling Lvl – 0.98
MFR & Balling Lvl – 0.98
Carb Flow & Balling Lvl – 0.95
Density & Balling Lvl – 0.95
Carb Rel & Balling Lvl – 0.85
Alch Rel & Balling Lvl – 0.92
Carb Rel & MFR – 0.83
Alch Rel & MFR – 0.90
Alch Rel & Balling – 0.90
Carb Rel & Balling – 0.83
Alch Rel & Carb Rel – 0.88
Balling & Balling Lvl – 0.98

#### Imputing and Cleaning the Test File Data

```{r}
## Doing the same type of processinfg for the Test data. Seeing if there are nulls etc. 
print(nrow(test)) # 267 Rows 

print(summary(test))
## Columns with highest number of nulls 
# - MFR 31 nulls 
# - Filler Speed. 10 NUlls 

## As it should be the PH variable is all null because this is what were predicting.
## Checking if these nulls are grouped together in a small number of rows? 
missing_per_row_test <- rowSums(is.na(test))
missing_summary_test <- as.data.frame(table(missing_per_row_test))
colnames(missing_summary_test) <- c("Missing_Count", "Row_Count")
print(missing_summary_test)

## Same rule of thumb for test data, if there are 4 or more nulls in a row they get dropped. Its a total of 6 rows that get dropped which is fine. 
test_cleaned <- test[missing_per_row_test < 4, ]

print(nrow(test_cleaned)) ## 261 rows left, so 6 rows dropped. 

# Brand COde needs to be converted from char. 
test_cleaned$`Brand Code` <- as.factor(test_cleaned$`Brand Code`)

```


```{r}
## Now imputing the test data to help predicitons.
init_test <- mice(test_cleaned, maxit = 0)
meth_test <- init_test$method  

print(meth_test)
meth_test$'Brand Code' <- "cart" # We want the categorical tree for this variable, not  a continuous numeric var. 
print(meth_test)

imputed_data_test <- mice(test_cleaned, method = meth_test, m = 10, seed = 100)
summary(imputed_data_test)

## Taking one fo the imputed datasets for now, if i want to use all of them alter i will. 
working_data_test <- complete(imputed_data_test, 1)

### Successfully got rid of all nulls. 
summary(working_data_test)

## Confirming it worked, specifically for brand code. 
unique(working_data_test$'Brand Code')
print(working_data_test |> filter(is.na(working_data_test$'Brand Code')))

```





```{r,fig.width=20, fig.height=20 }
## Taking a quick look at the relationships in the  data. 
numeric_data <- working_data %>% select_if(is.numeric)
cor_matrix <- cor(numeric_data, use = "pairwise.complete.obs")
#
#chart 
print(corrplot(cor_matrix, method = "color"))

```

```{r, message=FALSE, warning=FALSE, fig.width=20, fig.height=20}
# ---------------------- THIE PART MAY BE CUT, NOT SURE IF THE NUMBER IF VISUALS HERE IS WORTH IT ----------------------------------------------------

## Making pairs plot to get some prelimiinary insights into the predictor variables and their relationships. 
#print(ggpairs(training_cleaned))

# using column names to loop through and create smaller more readable pairs plots to example. However, notes that this will NOT ensure that every variable is compared to every other. IF they are not 
# plotted together, then they arent compared. This is to just get more of a sense fo the data overall. so decisions can be made. 
column_names <- names(working_data)
chunk_size <- 7
step_size <- chunk_size - 1  
starts <- seq(1, length(column_names) - step_size, by = step_size)

# Loop through overlapping chunks
for (start in starts) {
  end <- min(start + chunk_size - 1, length(column_names))
  vars_subset <- column_names[start:end]
  cat("\n\n### Plot for variables:", paste(vars_subset, collapse = ", "), "\n")
  print(ggpairs(working_data[, vars_subset],progress = FALSE) )
}


### Notes on these images: 
# - Carb Temp and Carb Pressure; Multicolinearity flag
# - Hyrdpressure1 and Mnf Flow, direct relationship. Maybe muilticolinearity flag? 
# - Density and Balling, potential Multicolinearity. 
#-- Some Variables dont have normal distributuions, but not too relevant for PLS regression. However, we should scale and center this data because of the multiple scales that the different vairables are on.

# However, these correlations dont matter much if we use a PLS regression, or some other non linear type regressions. Well Check PLS, then maybe do some aggregation of the directly related variables. 



```




## Data Analysis & Modeling 

Since the requirements are mainly to understand out manufacturing process. I will start with decision trees.
".. that new regulations are requiring us to *understand our manufacturing process, the predictive factors* and be able to report to them our predictive model of PH."



```{r,  fig.width=20, fig.height=20}

## Beginning to Model this data. PLS first.

## Need to split the training data, there are no PH values in the test data imported
split_index <- createDataPartition(working_data$PH, p = 0.8, list = FALSE)

train_data_split <- working_data[split_index, ]
test_data_split  <- working_data[-split_index, ]
train_data_split
```

#### Decision Trees

I will remove the highly correlated data 80%> correlation.


var1	var2	value
Balling.Lvl	Balling	0.9778224
Balling	Density	0.9551700
Balling.Lvl	Density	0.9476094

Bowl.Setpoint	Filler.Level	0.9404440
Hyd.Pressure3	Hyd.Pressure2	0.9254887

Alch.Rel	Balling	0.9237141
Balling.Lvl	Alch.Rel	0.9209349

MFR	Filler.Speed	0.9105181
Alch.Rel	Density	0.9011527
Carb.Rel	Alch.Rel	0.8437996

Balling.Lvl	Carb.Rel	0.8421633

Carb.Rel	Density	0.8205884
Carb.Rel	Balling	0.8192195
Carb.Temp	Carb.Pressure	0.8098785
Carb.Rel	Carb.Volume	0.7900296

```{r }
tree_train <- train_data_split %>%select(-c(Balling, "Filler Level", "Hyd Pressure2", Density,  "Filler Speed", "Carb Rel", "Carb Temp" ))

tree_model <- rpart(PH ~ ., 
                    data = tree_train, 
                    method="anova", # for regression
                    #method = "class",  # For classification
                    control = rpart.control(minsplit = 10, cp = 0.01))
```

variable importance

```{r}
tree_model$variable.importance
```

visualize the tree

```{r}
rpart.plot(tree_model, box.palette = "auto", nn = TRUE)
```




##### Describe Decision Tree

make predictions and evaluate
```{r}
predictions <- predict(tree_model, 
                       test_data_split %>%select(-c(PH)), 
                       type = "matrix")

#confusionMatrix(predictions, test2$Species)
```


```{r}
(tree_model_rmse <- RMSE(predictions, test_data_split$PH))

```
The RMSE is used to check how close estimates or forecasts are to actual values. Lower the MSE, the closer is
forecast to actual. This is used as a model evaluation measure for regression models and the lower value
indicates a better fit.

The Root Mean squared error (RMSE) represents the error of the estimator or predictive model created based on
the given set of observations in the sample. It measures the average squared difference between the predicted
values and the actual values, quantifying the discrepancy between the model’s predictions and the true
observations.

The lower RMSE, the better.


trying to prune the tree nd evaluate again

best xerror is 0.59854?
prune cp slightly higher than cp=0.010000
```{r}
printcp(tree_model)

```
```{r}
pruned_tree  <- prune(tree_model, cp = 0.011000)
#fancyRpartPlot(pruned_tree)
rpart.plot(pruned_tree, box.palette = "auto", nn = TRUE)

```
The pruned tree shows better performance ...

```{r}
predictions <- predict(pruned_tree, 
                       test_data_split %>%select(-c(PH)), 
                       type = "matrix")

(ptree_model_rmse <- RMSE(predictions, test_data_split$PH))

```

The root node displays the mean PH
```{r}
with(tree_train, round(mean(PH), 1))

```

The left node represents mean PH for Mnf.Flow >= -50

```{r}
with(tree_train, round(mean(PH[Mnf.Flow >=-50]), 1))
```

The right node represents mean PH for Mnf.Flow< -50

```{r}
with(tree_train, round(mean(PH[Mnf.Flow < -50]), 1))
#with(tree_train, mean(PH[Mnf.Flow < -50]))

```

node 4 is PH  for Mnf.Flow >= -50 and alcohol rel<7.5
```{r }
with(tree_train, round(mean(PH[(Mnf.Flow >= -50) & (Alch.Rel<7.5) ]), 1))

```

Showing predictor importance of our pruned tree.

```{r}
pruned_tree$variable.importance

```

####ADDITIONAL MODELS BELOW HERE



```{r}

## Crossvalidation set up first.
cross_val <- trainControl(method = "cv", number = 10)


#running PLS as before because a lot of predictors 
pls_model <- train(PH ~ ., data = train_data_split, method = "pls", preProc = c("center", "scale"),   tuneLength = 20, trControl = cross_val)

## Taking a look at the best model from the list
print(pls_model)
print(plot(pls_model))
## Taking Ncomp 11
## RMSE:0.1331814   R^2:0.4020757   MAE:  0.103935


#test_preds <- predict(pls_model, newdata = working_data_test, ncomp=22)
test_preds_pls <- predict(pls_model, newdata = test_data_split)
print(postResample(pred = test_preds_pls, obs = test_data_split$PH))

#     RMSE  Rsquared       MAE 
#0.1353367 0.3866006 0.1041878 

## Running it on the test data
#test_preds <- predict(pls_model, newdata = working_data_test, ncomp=22)


## Trying another model or two before we use these predictions 
#working_data_test$PH_Predicted <- test_preds

```




```{r}
## Results from the PLS Model were: 
#- RMSE: 0.1326951 
#- R^2:  0.4077471 
#- MAE:  0.1033471

## Giving additional models a try: 



## Trying MARS Model 
set.seed(100)
marsGrid <- expand.grid(.degree = 1:2,  .nprune = 2:50) 
marsFit <- train(PH ~ .,data = train_data_split,
                 method = "earth",
                 tuneGrid = marsGrid,
                 preProc = c("center", "scale"),
                 trControl = cross_val)


print(summary(marsFit$finalModel))

test_preds_mars <- predict(marsFit, newdata = test_data_split)
print(postResample(pred = test_preds_mars, obs = test_data_split$PH))
#      RMSE   Rsquared        MAE 
#0.12421475 0.48246207 0.09646806


## Trying SVM model
set.seed(100)
svmRTuned <- train(PH ~ .,data = train_data_split,
                   method = "svmRadial",
                   preProc = c("center", "scale"),
                   tuneLength = 30,
                   trControl = cross_val)

print(summary(svmRTuned$finalModel))

test_preds_svm <- predict(svmRTuned, newdata = test_data_split)
print(postResample(pred = test_preds_svm, obs = test_data_split$PH))
#      RMSE   Rsquared        MAE 
#0.11853567 0.53438002 0.08743236



## Trying Neural Net Model 
set.seed(100)
netGrid <- expand.grid(.decay = c(0, 0.01, 0.1),.size = 1:10,.bag = FALSE)
nnetTune <- train(PH ~ .,data = train_data_split,
                  method = "avNNet",  
                  tuneGrid = netGrid,
                  trControl = cross_val,
                  preProc = c("center", "scale"),
                  linout = TRUE,
                  trace = FALSE,
                  maxit = 500)
print(summary(nnetTune$finalModel))
test_preds_nn <- predict(nnetTune, newdata = test_data_split)
print(postResample(pred = test_preds_nn, obs = test_data_split$PH))
#
#

## Randomforest model
rf_model <- train(PH ~ .,data = working_data, method = "rf",  trControl = cross_val,  importance = TRUE)
print(rf_model)
```


## Conclusion

#### From Deciion Tree Model 

The greatest predictor importance is to Mnf.Flow.

The pruned tree model produces the lowest RMSE of 0.1284586.

The model is simple and created just by removing predictors with high correlation.

```{r}
```



```{r}
```

