---
title: "DATA621_FinalProject"
author: "Group2"
date: "2025-05-08"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("glmmTMB")
library(lme4)
library(httr)
library(jsonlite)
library(dplyr)
library(lubridate)
library(stringr)
library(tidyr)
library(GGally)
library(sf)
library(caret)
library(glmmTMB)
library(DHARMa)

```

## Data Acquisition

#### NYC 311 Data 

```{r}

#Starting with 311 Data; limiting to road / bridge other automobile infrastructure complaints via dot conditions

# # API work for pulling in 311 data
# base_url <- "https://data.cityofnewyork.us/resource/erm2-nwe9.json"
# rows_per_request <- 1000
# offset <- 0
# all_data <- list()
# done <- FALSE
# 
# # Loop for pagination from 2019 through 2024 (5 Years of data)
# start_date <- paste0(2019, "-01-01T00:00:00")
# end_date <- paste0(2024 + 1, "-01-01T00:00:00")
# 
# 
# ### Pulling in the 311 d
# while (!done) {
#   query <- list(
#   "$where" = paste0(
#     "created_date >= '", start_date,
#     "' AND created_date < '", end_date,
#     "' AND agency_name = 'Department of Transportation'",
#     " AND complaint_type like '%Condition%'",
#     " AND incident_zip IS NOT NULL"
#   ),
#   "$limit" = rows_per_request,
#   "$offset" = offset
#   )
#   print(offset)
#   response <- GET(base_url, query = query)
#   data_chunk <- fromJSON(content(response, as = "text", encoding = "UTF-8"))
# 
#   if (length(data_chunk) ==0) {
#     done <- TRUE
#   } else {
#     all_data <- append(all_data, list(data_chunk))
#     offset <- offset + rows_per_request
#     Sys.sleep(2)
#   }
# }
# 
# 
# #first_attempt <-df_311
# # Combine into one data frame
# df_311 <- bind_rows(all_data)

## Saving to  a file to pick up later if needed.
#saveRDS(df_311, "dot_condition_311_requests_2019_2024.rds")
df_311 <- readRDS("dot_condition_311_requests_2019_2024.rds")

```
#### NYC Vehicle Accident Data 

```{r}
# Set API endpoint
# base_url <- "https://data.cityofnewyork.us/resource/h9gi-nx95.json"
# 
# # Same Date limits, resetting the offsets and other vars
# offset <- 0
# all_data <- list()
# done <- FALSE
# 
# # Loop for pagination from 2019 through 2024 (5 Years of data)
# start_date <- paste0(2019, "-01-01T00:00:00")
# end_date <- paste0(2024 + 1, "-01-01T00:00:00")
# 
# # Pulling loop
# while (!done) {
#   query <- list(
# "$where" = paste0(
#     "crash_date >= '", start_date,
#     "' AND crash_date < '", end_date, 
#     "' AND zip_code IS NOT NULL"
#   ),    "$limit" = rows_per_request,
#     "$offset" = offset
#   )
#   print(offset)
#   response <- GET(base_url, query = query)
#   data_chunk <- fromJSON(content(response, as = "text", encoding = "UTF-8"))
# 
#   if (length(data_chunk) == 0) {
#     done <- TRUE
#   } else {
#     all_data <- append(all_data, list(data_chunk))
#     offset <- offset + rows_per_request
#     Sys.sleep(2)  
#   }
# }
# 
# # Combine and save
# df_crashes <- bind_rows(all_data)


## Savign to  a file to pick up later if needed.
#saveRDS(df_crashes, "motor_vehicle_crashes_2019_2024.rds")
df_crashes <- readRDS("motor_vehicle_crashes_2019_2024.rds")

```

#### Pulling in the Shapefile and ZCTA data
```{r}
## Shapefile 
zcta <- st_read("https://data.cityofnewyork.us/resource/pri4-ifjk.geojson")
#zcta
## Creating a crosswalk tro zipcodes to zctas, zipcodes not real geographies. Need the zctas.
crosswalk_df <- zcta %>%
  filter(!is.na(label)&(label!="")) %>%
  mutate(zip = str_split(label, ",")) %>%
  unnest(zip) %>%
  mutate(zip = str_trim(zip))|>
  select(modzcta,zip)
crosswalk_df <- distinct(crosswalk_df)
crosswalk_df <- crosswalk_df %>%  rename(zip_code = zip) %>%  mutate(zip_code = as.character(zip_code))
crosswalk_df<- st_drop_geometry(crosswalk_df)

print(head(crosswalk_df))
```

#### Processing the 311 Data
```{r}
###Processing the 311 data

df_311_clean <- df_311[!is.na(df_311$agency), ]
#nrow(df_311_clean) #1028706

## Parse out Dates (I think by month is ideal )
df_311_clean$created_date <- ymd_hms(df_311_clean$created_date)

#Checking the dates here, Summary vals seems to be fine,
#summary(df_311_clean$created_date)


df_311_clean$year_month <- floor_date(df_311_clean$created_date, unit = "month")

## Getting the amount of time the complaint was open for

#nrow(df_311_clean %>%
  #filter(is.na(closed_date))) #16,291 null values for close date out of #1,028,706 total rows (~1.5%)

## Can just drop these. 
df_311_clean <- df_311_clean %>%  filter(!is.na(closed_date))

## Checking the new total number of rows 
print(nrow(df_311_clean)) #1,012,415

## Converting to date for closed date and getting the number of days a ticket is open for.
df_311_clean$closed_date <- ymd_hms(df_311_clean$closed_date)

## There is a min date in year 1899. Checking to see how pronounced the issue is.
summary(df_311_clean$closed_date)

## none of the closed dates should be less than the created_dates (whos values i checked earlier)
earlier_closed_date <- df_311_clean %>%  filter(closed_date < created_date)
print(nrow(earlier_closed_date))  # 38,013 (3.7% of the df) 
print(table(earlier_closed_date$status)) ## Most (37,801) of these entries have "pending" status. 

## I think we drop because of these being stagnant type entries with corrupted date ranges. 
df_311_clean <- df_311_clean %>%  filter(!closed_date < created_date)

## Noew adding column for days tickets are open for. 
df_311_clean <- df_311_clean %>%  mutate(days_open = as.numeric(difftime(closed_date, created_date, units = "days")))

#Checking Results
sum(is.na(df_311_clean$days_open)) ## no nulls
sum(df_311_clean$days_open < 0) ## no neg values

## Prelim glance of values. 
# print(hist(df_311_clean$days_open))

## Limiting to the columns we want for our analysis
df_311_limited <- df_311_clean %>%  select(unique_key, year_month, borough, community_board, incident_zip,
                                           address_type,days_open, status, complaint_type, descriptor, resolution_description)

## numer of rows for working df 
#print(nrow(df_311_limited)) #974,402; just under a million. This is ~94.7% of the original data.

### Looking at new limited manageable df 
print(head(df_311_limited))

# Basic structure and types; All chr as they should be.
print(str(df_311_limited))

# Summary statistics
print(summary(df_311_limited))

## Getting NA values
print(colSums(is.na(df_311_limited)))

## 2 nulls in borough we should deall with
## 2 Nulls in community board
## Address type has a lot (140,707)
## 192 Nulls for resolution description. 
## The rest are ok. 

#Dealing with null instances in the borough and cb columns, i dont think the other columns matter much.

## Manually looking up the two zip codes and what borough they are in. ALso doing for community boards via zip
print(df_311_limited[is.na(df_311_limited$borough), ])

# 10305 == STATEN ISLAND == 01 STATEN ISLAND
# 10301 == STATEN ISLAND == 01 STATEN ISLAND

df_311_limited$borough[
  is.na(df_311_limited$borough) & 
  df_311_limited$incident_zip %in% c("10305", "10301")
] <- "STATEN ISLAND"

print(df_311_limited[is.na(df_311_limited$community_board), ])

df_311_limited$community_board[
  is.na(df_311_limited$community_board) & 
  df_311_limited$incident_zip %in% c("10305", "10301")
] <- "01 STATEN ISLAND"

##Checking the CB Values,
print(unique(df_311_limited$community_board))

# No more nulls, but here are values for "unspecified".
# Splitting the df into two, with specified cb and not specified CB. 

df_unspecified_cb <- df_311_limited %>%
  filter(grepl("Unspecified", community_board, ignore.case = TRUE))

df_specified_cb <- df_311_limited %>%
  filter(!grepl("Unspecified", community_board, ignore.case = TRUE))


# Attempting to Enrich the unspecified values with the specific values from the df_specifcied_cb df

# Checking if CB to zip is proper (checking that zip codes dont straddle more than one cb)
zip_cb_counts <- df_specified_cb %>%
  group_by(incident_zip) %>%
  summarize(unique_cb_count = n_distinct(community_board)) %>%
  arrange(desc(unique_cb_count))

zip_cb_counts %>% filter(unique_cb_count > 1)

## There are zip codes that seem to stradle more than one CB. So, dropping CB column, not worth keeping. Will jsut work with Borough and Zip.
## Community Board is also not present in vehicle accident data so doesnt really matter.

## Dropping Cb columns
df_311_limited <- df_311_limited %>% select(-community_board)

## Dropping the address_type, and resolution_description  (Can come back and un do if we want to later)
df_311_limited <- df_311_limited %>% select(-address_type)
df_311_limited <- df_311_limited %>% select(-resolution_description )

## Categorizing the Days open into Groups
df_311_limited <- df_311_limited %>%
  mutate(days_open_group = case_when(
    days_open < 7             ~ "open_less_than_1_week",
    days_open >= 7 & days_open < 30  ~ "open_less_than_1_month",
    days_open >= 30           ~ "open_a_month_or_more"
  ))


### Looking at the complaint types
print(unique(df_311_limited$complaint_type))


## only want to keep the ones that pertain to driving
df_311_limited <- df_311_limited %>%
  filter(complaint_type %in% c('Street Condition',"Traffic Signal Condition","Street Light Condition","Highway Condition",
                              "Bridge Condition","Tunnel Condition","DEP Street Condition","DEP Highway Condition"))

### limiting to three Categories
df_311_limited <- df_311_limited %>%
  mutate(complaint_group_custom = case_when(
    complaint_type %in% c("Street Condition", "Highway Condition", "Bridge Condition", "Tunnel Condition") ~ "road_infra_based_complaint",
    complaint_type %in% c("DEP Street Condition", "DEP Highway Condition") ~ "dep_infra_based_complaint",
    complaint_type %in% c("Traffic Signal Condition", "Street Light Condition") ~ "light_signal_based_complaint",
    TRUE ~ "other"
  ))


print(nrow(df_311_limited))#772,470
## adding in zcta, inner join to keep valid zip codes
df_311_limited <- df_311_limited %>%  rename(zip_code = incident_zip)
df_311_limited <- df_311_limited %>% inner_join(crosswalk_df, by = "zip_code")
print(nrow(df_311_limited)) #769,708



## Getting the total complaints with leaving gthe columns we care abouts
agg_311 <- df_311_limited %>%
  group_by(year_month, modzcta, complaint_group_custom, days_open_group) %>%
  summarize(complaint_count = n_distinct(unique_key), .groups = "drop")

print(head(agg_311))

### I think we still need to make wider to flatten out wide-ways for regression and for smooth join with accdient data

## making the two complaint categories into one and then making them each a column
agg_311 <- agg_311 %>%
  mutate(complaint_categories = paste(complaint_group_custom, days_open_group, sep = "_"))|>
  select(year_month, modzcta,complaint_categories, complaint_count)

agg_311_wide <- agg_311 %>%
  pivot_wider(
    names_from = complaint_categories,
    values_from = complaint_count,
    values_fill = 0 )

## THis DF if for regression work to join in semi final with crash
print(head(agg_311_wide))

```


#### 311 Mapping Visualizations

```{r, fig.height=12, fig.width=15}
## Making additional DFs for demonstrative maps of complaints by zip
agg_311_comps <- df_311_limited %>%
   group_by(year_month, modzcta) %>%
   summarize(complaint_count = n_distinct(unique_key), .groups = "drop")
 
## Annual Averages by zip fro simple maping 
annual_tot_complaints <- agg_311_comps %>%
  mutate(year = year(year_month)) %>%
  group_by(modzcta, year) %>%
  summarise(total_complaints = sum(complaint_count), .groups = "drop")

annual_avg_by_days_open <- df_311_limited %>%
  mutate(year = year(year_month)) %>%
  group_by(modzcta, year) %>%
  summarise(avg_days_open = mean(days_open), .groups = "drop")


## Joining zcta geog
complaints_map_df <- left_join(zcta, annual_tot_complaints, by = "modzcta")
complaints_map_df <- complaints_map_df |> filter(!is.na(complaints_map_df$year))

complaints_days_map_df <- left_join(zcta, annual_avg_by_days_open, by = "modzcta")
complaints_days_map_df <- complaints_days_map_df |> filter(!is.na(complaints_days_map_df$year))

##omplaints by zip
p<-ggplot(complaints_map_df) +
  geom_sf(aes(fill = total_complaints), color = "white") +
  scale_fill_viridis_c(option = "inferno", direction = -1, name = "Total Complaints", na.value = "grey90") +
  labs(title = "Yearly Total 311 DOT Road-Focused Complaints by ZCTA (2019–2024)") +
  facet_wrap(~ year) +
  theme_minimal() +
  theme(
    #plot.title = element_text(hjust = 0.5),
    panel.grid = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    axis.title = element_blank(),
    legend.key.height = unit(0.8, "cm"),  
    legend.text = element_text(size = 10),   
    legend.title = element_text(size = 11)  
  )

ggsave("./images/311ComplaintsByZCTA.png", plot = p)

#Days open by zcta
p<-ggplot(complaints_days_map_df) +
  geom_sf(aes(fill = avg_days_open), color = "white") +
  scale_fill_viridis_c(
    option = "magma",
    direction = -1,
    breaks = c(0, 7, 14, 30, 60),
    labels = c("0", "7", "14", "30", "60+"),
    limits = c(0, 60),
    name = "Avg Days Open",
    na.value = "grey90"
  ) +
  labs(
    title = "Average Days A Complaint Stays Open by ZCTA (2019–2024)"
  ) +
  facet_wrap(~ year) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5),
    panel.grid = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    axis.title = element_blank(),
    legend.key.height = unit(0.8, "cm"),     
    legend.text = element_text(size = 10),   
    legend.title = element_text(size = 11)   
  )
ggsave("./images/DaysOpenByZCTA.png", plot = p)

```

#### Processing Accident Data
``` {r}
## number of vehicles involved in an accident
df_crashes_mod <- df_crashes %>%
  mutate(
      vehicle_1 = coalesce(vehicle_type_code1, contributing_factor_vehicle_1),
      vehicle_2 = coalesce(vehicle_type_code2, contributing_factor_vehicle_2),
      vehicle_3 = coalesce(vehicle_type_code_3, contributing_factor_vehicle_3),
      vehicle_4 = coalesce(vehicle_type_code_4, contributing_factor_vehicle_4),
      vehicle_5 = coalesce(vehicle_type_code_5, contributing_factor_vehicle_5)) %>%
  mutate(
      vehicles_involved= rowSums(!is.na(select(., vehicle_1, vehicle_2, vehicle_3, vehicle_4, vehicle_5))),
    ) %>%  select(crash_date,collision_id, borough, zip_code, number_of_persons_injured, number_of_persons_killed, vehicles_involved)

# Seeing Results
#hist(df_crashes_mod$vehicles_involved)

# There are zeros which makes no sense, so we're going to make the zeros have a 1 value. I'm assuming its a data entry error.
df_crashes_mod <- df_crashes_mod %>%  mutate(vehicles_involved = ifelse(vehicles_involved == 0, 1, vehicles_involved))

#formatting dates
df_crashes_mod$crash_date <- ymd_hms(df_crashes_mod$crash_date)


## Limiting to the columns we want to keep. 
df_crashes_lim <- df_crashes_mod |> select(crash_date, collision_id, borough, zip_code, number_of_persons_injured, number_of_persons_killed, vehicles_involved) 

## Enriching with zctas, the actual geogs.
print(nrow(df_crashes_lim))#478,560
## adding in zcta, inner join to keep valid zip codes
df_crashes_lim <- df_crashes_lim %>% inner_join(crosswalk_df, by = "zip_code")
print(nrow(df_crashes_lim)) #477,287
print(head(df_crashes_lim))

## Extracting month year
df_crashes_lim$year_month <- floor_date(df_crashes_lim$crash_date, unit = "month")

#Ranking Crashes (PASSING ON THIS FOR NOW)
# df_crashes_lim<- df_crashes_lim %>%
#   mutate(crash_severity = case_when(
#     number_of_persons_killed > 0 ~ "Fatal",
#     number_of_persons_injured > 0 ~ "Injury",
#     vehicles_involved >= 3 ~ "Multi-Vehicle",
#     TRUE ~ "Minor"
#   ))


## Group by to get counts
agg_crash <- df_crashes_lim %>%
  group_by(year_month, modzcta) |> 
  summarize(accident_count = n_distinct(collision_id), .groups = "drop")

print(head(agg_crash))

## For now i want one consistent target variable, which would be number of accidents so getting the total column back in without categories.
## i think accident categories were over complicating it.

## Final Transpose / widening of table before join 
#agg_crash <- agg_crash %>%
#  mutate(crash_severity = if_else(crash_severity == "Multi-Vehicle", "Multi_Vehicle", crash_severity))


#agg_crash_wide <- agg_crash %>%
#  pivot_wider(
#    names_from = crash_severity,
#    values_from = accident_count,
#    values_fill = 0 )



```


#### Accident Map Visuals

```{r, fig.height=12, fig.width=15}

annual_tot_crash <- agg_crash %>%
  mutate(year = year(year_month)) %>%
  group_by(modzcta, year) %>%
  summarise(total_accidents = sum(accident_count), .groups = "drop")

accidents_map_df <- left_join(zcta, annual_tot_crash, by = "modzcta")
accidents_map_df <- accidents_map_df |> filter(!is.na(accidents_map_df$year))

p<-ggplot(accidents_map_df) +
  geom_sf(aes(fill = total_accidents), color = "white") +
  scale_fill_viridis_c(option = "inferno",direction = -1, name = "Total Car Accidents", na.value = "grey90") +
  labs(title = "Yearly Total Car Accidents by ZCTA (2019–2024)") +
  facet_wrap(~ year) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5),
    panel.grid = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    axis.title = element_blank(),
    legend.key.height = unit(0.8, "cm"),  
    legend.text = element_text(size = 10),   
    legend.title = element_text(size = 11)  
  )
ggsave("./images/AccidentsByZCTA.png", plot = p)
```


#### Working Dataset Creation
```{r}

## Joining both processed datasets into 1, also adding Zip Neighborhood info from the ZCTA data 
semifinal_data <- full_join(agg_crash,  agg_311_wide,  by = c("year_month", "modzcta"))

### Drilling into one month , zip etc. 
#semifinal_data |> filter(modzcta == '11229',
#                          year_month == as.Date("2020-03-01"))


### Lastg Mile clean up for Borough and Zip 

print(nrow(semifinal_data)) # 12,634 rows 

# We need a zip code value
semifinal_data <- semifinal_data |>  filter(!(is.na(modzcta)))


print(nrow(semifinal_data)) # 12,634  rows  (dropped a null row for zip)


## Filling in the nulls w2ith zeros
semifinal_data_full <- semifinal_data
semifinal_data_full[is.na(semifinal_data_full)] <- 0
print(head(semifinal_data_full))


```


### Additional Exploratory Work

#### Basic Level Statistics
```{r}
summary(semifinal_data_full)
```

#### Visualizations & Final Pre-Modeling Steps
```{r,message=FALSE, warning=FALSE,fig.height=20, fig.width=25}

## Histogram for all Vars 
p<-semifinal_data_full %>%
  select(where(is.numeric)) %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value") %>%
  ggplot(aes(x = value)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "white") +
  facet_wrap(~variable, scales = "free", ncol = 3) +
  theme_minimal() +
  theme(strip.text = element_text(size = 5),       
        strip.text.x = element_text(angle = 0),
        plot.title = element_text(hjust = 0.5))+
  labs(title = "Histograms of Complaint Category Variables", x = "", y = "Count")

ggsave("./images/HistogramRawVariables.png", plot = p)

## Dropping those that are essentially always zero. CHecking their variance before dropping 
nzv <- nearZeroVar(semifinal_data_full, saveMetrics = TRUE)
vars_to_drop <- rownames(nzv[nzv$nzv == TRUE, ])
print(vars_to_drop) # Its the DEP complaints
semifinal_data_pruned <- semifinal_data_full %>% select(-all_of(vars_to_drop))

## Dropped NZV columns, next histogram. 
#p<-
semifinal_data_pruned %>%
  select(where(is.numeric)) %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value") %>%
  ggplot(aes(x = value)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "white") +
  facet_wrap(~variable, scales = "free", ncol = 3) +
  theme_minimal() +
  theme(strip.text = element_text(size = 5),       
        strip.text.x = element_text(angle = 0),
        plot.title = element_text(hjust = 0.5))+
  labs(title = "Histograms of Complaint Category Variables (After NZV Drop)", x = "", y = "Count")

#ggsave("./images/HistogramRawVariables_noNZV.png", plot = p)

## GGpais plot to check it out this way.
#p<-
ggpairs(semifinal_data_pruned %>% select(-modzcta, -year_month))+
    theme(
  axis.text.x = element_text(angle = 45, hjust = 1, size = 12),  
  axis.text.y = element_text(size = 10)
)
#ggsave("./images/GGPairs_noNZV.png", plot = p, width = 25, height = 25, dpi = 300)


```

#### Log Transforming Data 
```{r message=FALSE, warning=FALSE, results='hide',fig.height=12, fig.width=15}
## After reviewing the data we need to transform these values. Trying via log(x +1)
cols_to_transform <- c(
  "accident_count",
  "light_signal_based_complaint_open_less_than_1_week",
  "road_infra_based_complaint_open_less_than_1_month",
  "road_infra_based_complaint_open_less_than_1_week",
  "light_signal_based_complaint_open_less_than_1_month",
  "light_signal_based_complaint_open_a_month_or_more",
  "road_infra_based_complaint_open_a_month_or_more"
  )

# Log(x+1) 
semifinal_data_log <- semifinal_data_pruned %>%
  mutate(across(all_of(cols_to_transform), ~ log(. + 1)))

## Transformed Hist plots to check dist.
p<-semifinal_data_log %>%
  select(where(is.numeric)) %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value") %>%
  ggplot(aes(x = value)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "white") +
  facet_wrap(~variable, scales = "free", ncol = 3) +
  theme_minimal() +
  theme(strip.text = element_text(size = 5),       
        strip.text.x = element_text(angle = 0),
        plot.title = element_text(hjust = 0.5))+
  labs(title = "Histograms of Log(x+1) Transformed Variables", x = "", y = "Count")

ggsave("./images/HistogramLogVariables.png", plot = p)


##ggpairs with transformed
p<-ggpairs(semifinal_data_log %>% select(-modzcta, -year_month))+
    theme(
  axis.text.x = element_text(angle = 45, hjust = 1, size = 10),  
  axis.text.y = element_text(size = 10)
)
ggsave("./images/GGPairsLogTrans_noNZV.png", plot = p, width = 25, height = 25, dpi = 300)

```

#### Square Root Transforming Data
```{r}

# Sqrt(x+1)
semifinal_sqrt <- semifinal_data_pruned %>%
  mutate(across(all_of(cols_to_transform), ~ sqrt(.+1)))

p<-semifinal_sqrt %>%
  select(where(is.numeric)) %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value") %>%
  ggplot(aes(x = value)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "white") +
  facet_wrap(~variable, scales = "free", ncol = 3) +
  theme_minimal() +
  theme(strip.text = element_text(size = 5),       
        strip.text.x = element_text(angle = 0),
        plot.title = element_text(hjust = 0.5))+
  labs(title = "Histograms of Square Root (x+1) Transformed Variables", x = "", y = "Count")

ggsave("./images/HistogramSqRtVariables.png", plot = p)

##ggpairs with transformed
p<-ggpairs(semifinal_sqrt %>% select(-modzcta, -year_month))+
    theme(
  axis.text.x = element_text(angle = 45, hjust = 1, size = 10),  
  axis.text.y = element_text(size = 10)
)
ggsave("./images/GGPairsSqRtTrans_noNZV.png", plot = p, width = 25, height = 25, dpi = 300)


```

#### Cube-Root Transforming Data
```{r}

# x+1^(1/3)
semifinal_cuberoot <- semifinal_data_pruned %>%
  mutate(across(all_of(cols_to_transform), ~ (.+1)^(1/3)))


p<-semifinal_cuberoot %>%
  select(where(is.numeric)) %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value") %>%
  ggplot(aes(x = value)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "white") +
  facet_wrap(~variable, scales = "free", ncol = 3) +
  theme_minimal() +
  theme(strip.text = element_text(size = 5),       
        strip.text.x = element_text(angle = 0),
        plot.title = element_text(hjust = 0.5))+
  labs(title = "Histograms of (x+1)^1/3  Transformed Variables", x = "", y = "Count")

ggsave("./images/HistogramCubedRtVariables.png", plot = p)

##ggpairs with transformed
p<-ggpairs(semifinal_cuberoot %>% select(-modzcta, -year_month))+
    theme(
  axis.text.x = element_text(angle = 45, hjust = 1, size = 10),  
  axis.text.y = element_text(size = 10)
)
ggsave("./images/GGPairsCubedRtTrans_noNZV.png", plot = p, width = 25, height = 25, dpi = 300)

### Some variables seem to be more normal with a cube or log transform. Most predictors are still heavily skewed regardless of transform. Also all of the data is zero inflated and positive, so this needs to be considered for the regression. Testing both cubed root and Logged transformed data for over dispersion to help choose a model 

```

#### Overdispersion check
```{r message=FALSE, warning=FALSE,}
### Checking log transformed data for over disperson. 
### Executing a Poisson Regression with the log transofmration to then confirm there is no over dispersion before i use ZIP regression
pois_model <- glmer(accident_count ~ 1 + (1 | modzcta), family = poisson, data = semifinal_data_log)
overdisp_fun <- function(model) {
  rdf <- df.residual(model)
  rp <- residuals(model, type = "pearson")
  Pearson.chisq <- sum(rp^2)
  prat <- Pearson.chisq / rdf
  c(chisq = Pearson.chisq, ratio = prat, rdf = rdf)
}

print(overdisp_fun(pois_model))

#       chisq        ratio          rdf 
#1.102236e+03 8.725742e-02 1.263200e+04 

## LOGGED DATA: No Overdispersion, so ZIP regression
## ----------------------------------------------------------

### Executing a Poisson Regression with the cubed root transofmration to then confirm there is no over dispersion before i use ZIP regression
pois_model <- glmer(accident_count ~ 1 + (1 | modzcta), family = poisson, data = semifinal_cuberoot)
overdisp_fun <- function(model) {
  rdf <- df.residual(model)
  rp <- residuals(model, type = "pearson")
  Pearson.chisq <- sum(rp^2)
  prat <- Pearson.chisq / rdf
  c(chisq = Pearson.chisq, ratio = prat, rdf = rdf)
}

print(overdisp_fun(pois_model))
 # chisq        ratio          rdf 
# 8.134156e+02 6.439326e-02 1.263200e+04

## CUBED ROOT DATA: No Overdispersion, so also ZIP regression
```

### ZIP Regression Selection Needs Integer Target, with Transformed Predictors

```{r}
## Zip regression needs target vars to be int, so need to not have it transformed, but still transform the predictors. 
cols_to_transform_pred <- c(
  "light_signal_based_complaint_open_less_than_1_week",
  "road_infra_based_complaint_open_less_than_1_month",
  "road_infra_based_complaint_open_less_than_1_week",
  "light_signal_based_complaint_open_less_than_1_month",
  "light_signal_based_complaint_open_a_month_or_more",
  "road_infra_based_complaint_open_a_month_or_more"
  )

semifinal_data_log_pred <- semifinal_data_pruned %>%  mutate(across(all_of(cols_to_transform_pred), ~ log(. + 1)))
semifinal_data_cbrt_pred <- semifinal_data_pruned %>%  mutate(across(all_of(cols_to_transform_pred), ~ (. + 1)^(1/3)))
```

### Modeling
#### Time Var. As Random 
##### Zero Inflated Regression with Logged Predictor Data
```{r}
### MODELING LOG DATA WITH Zero Inflated Poisson

## Also need to alter time column for index
semifinal_data_log_pred <- semifinal_data_log_pred[order(semifinal_data_log_pred$modzcta, semifinal_data_log_pred$year_month), ]
semifinal_data_log_pred$time_index <- as.numeric(as.factor(semifinal_data_log_pred$year_month))

model_zip <- glmmTMB(
  accident_count ~ 
    time_index +
    light_signal_based_complaint_open_less_than_1_week +
    light_signal_based_complaint_open_less_than_1_month +
    light_signal_based_complaint_open_a_month_or_more +
    road_infra_based_complaint_open_less_than_1_week +
    road_infra_based_complaint_open_less_than_1_month +
    road_infra_based_complaint_open_a_month_or_more +
    (1 | modzcta),               
  ziformula = ~1,                 
  family = poisson,
  data = semifinal_data_log_pred
)

summary(model_zip)



```

#### Zero Inflated Regression with Cube-Root Predictor Data
```{r}
### MODELING Cube Root DATA WITH Zero Inflated Poisson
## Also need to alter time column for index
semifinal_data_cbrt_pred <- semifinal_data_cbrt_pred[order(semifinal_data_cbrt_pred$modzcta, semifinal_data_cbrt_pred$year_month), ]
semifinal_data_cbrt_pred$time_index <- as.numeric(as.factor(semifinal_data_cbrt_pred$year_month))

model_zip_cb <- glmmTMB(
  accident_count ~ 
    time_index +
    light_signal_based_complaint_open_less_than_1_week +
    light_signal_based_complaint_open_less_than_1_month +
    light_signal_based_complaint_open_a_month_or_more +
    road_infra_based_complaint_open_less_than_1_week +
    road_infra_based_complaint_open_less_than_1_month +
    road_infra_based_complaint_open_a_month_or_more +
    (1 |modzcta),                # Random intercept per ZIP code
  ziformula = ~1,                 # Constant zero-inflation probability
  family = poisson,
  data = semifinal_data_cbrt_pred
)

summary(model_zip_cb)
```
### Digging into the Statistically Significant Predictor Results 
```{r}
print("light_signal_based_complaint_open_less_than_1_week")
print((exp(2.787e-02)-1)*100)

print("light_signal_based_complaint_open_less_than_1_month")
print((exp(1.735e-01)-1)*100)

print("light_signal_based_complaint_open_a_month_or_more")
print((exp(-9.899e-02)-1)*100)

print("road_infra_based_complaint_open_less_than_1_week")
print((exp(1.591e-01)-1)*100)

print("road_infra_based_complaint_open_less_than_1_month")
print((exp(4.456e-02)-1)*100)

```

### Residual Check for Cube-Root ZIP model
```{r}
residuals_zip <- residuals(model_zip_cb)
fitted_vals <- fitted(model_zip_cb)


plot(fitted_vals, residuals_zip,
     xlab = "Fitted values", ylab = "Residuals",
     main = "Residuals vs Fitted")

plot(semifinal_data_cbrt_pred$time_index, residuals_zip,
     xlab = "Time index", ylab = "Residuals",
     main = "Residuals over Time")

hist(residuals_zip, breaks = 30, main = "Histogram of Residuals",
     xlab = "Residuals")

qqnorm(residuals_zip)
```

### Testing the MOdel On 2025 Data

```{r}
## Pulling in 2025 Data from 311
base_url <- "https://data.cityofnewyork.us/resource/erm2-nwe9.json"
rows_per_request <- 1000
offset <- 0
all_data <- list()
done <- FALSE

# Loop for pagination for 2025
start_date <- paste0(2025, "-01-01T00:00:00")
end_date <- paste0(2025 + 1, "-01-01T00:00:00")


### Pulling in the 311 d
while (!done) {
  query <- list(
  "$where" = paste0(
    "created_date >= '", start_date,
    "' AND created_date < '", end_date,
    "' AND agency_name = 'Department of Transportation'",
    " AND complaint_type like '%Condition%'",
    " AND incident_zip IS NOT NULL"
  ),
  "$limit" = rows_per_request,
  "$offset" = offset
  )
  print(offset)
  response <- GET(base_url, query = query)
  data_chunk <- fromJSON(content(response, as = "text", encoding = "UTF-8"))

  if (length(data_chunk) ==0) {
    done <- TRUE
  } else {
    all_data <- append(all_data, list(data_chunk))
    offset <- offset + rows_per_request
    Sys.sleep(2)
  }
}

#first_attempt <-df_311
# Combine into one data frame
df_311_2025 <- bind_rows(all_data)

# Savign to  a file to pick up later if needed.
saveRDS(df_311_2025, "dot_condition_311_requests_2025.rds")
# df_311_2025 <- readRDS("dot_condition_311_requests_2025.rds")
```

```{r}
## Processing 311 2025 Data the Same way
###Processing the 311 data

df_311_2025_clean <- df_311_2025[!is.na(df_311_2025$agency), ]
#nrow(df_311_2025_clean) #68071

## Parse out Dates (I think by month is ideal )
df_311_2025_clean$created_date <- ymd_hms(df_311_2025_clean$created_date)

#Checking the dates here, Summary vals seems to be fine,
#summary(df_311_2025_clean$created_date)


df_311_2025_clean$year_month <- floor_date(df_311_2025_clean$created_date, unit = "month")

## Getting the amount of time the complaint was open for

#nrow(df_311_2025_clean %>%
 # filter(is.na(closed_date))) #6,014 null values for close date out of #68,071 total rows (~8.8%)

## Can just drop these. 
df_311_2025_clean <- df_311_2025_clean %>%  filter(!is.na(closed_date))

## Checking the new total number of rows 
print(nrow(df_311_2025_clean)) #62057

## Converting to date for closed date and getting the number of days a ticket is open for.
df_311_2025_clean$closed_date <- ymd_hms(df_311_2025_clean$closed_date)

## There is a min date in 2024. Digging deeper
#summary(df_311_2025_clean$closed_date)

## none of the closed dates should be less than the created_dates (whos values i checked earlier)
earlier_closed_date_2025 <- df_311_2025_clean %>%  filter(closed_date < created_date)
print(nrow(earlier_closed_date_2025))  # 393 (0.6% of the df) 
print(table(earlier_closed_date_2025$status)) ## Most (391) of these entries have "pending" status. 

## I think we drop because of these being stagnant type entries with corrupted date ranges. 
df_311_2025_clean <- df_311_2025_clean %>%  filter(!closed_date < created_date)

## Noew adding column for days tickets are open for. 
df_311_2025_clean <- df_311_2025_clean %>%  mutate(days_open = as.numeric(difftime(closed_date, created_date, units = "days")))

#Checking Results
sum(is.na(df_311_2025_clean$days_open)) ## no nulls
sum(df_311_2025_clean$days_open < 0) ## no neg values

## Prelim glance of values. 
#print(hist(df_311_2025_clean$days_open))

## Limiting to the columns we want for our analysis
df_311_2025_limited <- df_311_2025_clean %>%  select(unique_key, year_month, borough, community_board, incident_zip,
                                           address_type,days_open, status, complaint_type, descriptor, resolution_description)

## number of rows for working df 
#print(nrow(df_311_2025_limited)) #61,664. This is ~90% of the original data.

### Looking at new limited manageable df 
print(head(df_311_2025_limited))

# Basic structure and types; All chr as they should be.
print(str(df_311_2025_limited))

# Summary statistics
print(summary(df_311_2025_limited))

## Getting NA values
print(colSums(is.na(df_311_2025_limited)))

## Columns we need are fine. Dropping others.
## Dropping Cb columns
df_311_2025_limited <- df_311_2025_limited %>% select(-community_board)
## Dropping the address_type, and resolution_description  (Can come back and un do if we want to later)
df_311_2025_limited <- df_311_2025_limited %>% select(-address_type)
df_311_2025_limited <- df_311_2025_limited %>% select(-resolution_description )

## Categorizing the Days open into Groups
df_311_2025_limited <- df_311_2025_limited %>%
  mutate(days_open_group = case_when(
    days_open < 7             ~ "open_less_than_1_week",
    days_open >= 7 & days_open < 30  ~ "open_less_than_1_month",
    days_open >= 30           ~ "open_a_month_or_more"
  ))


### Looking at the complaint types
print(unique(df_311_2025_limited$complaint_type))


## only want to keep the ones that pertain to driving
df_311_2025_limited <- df_311_2025_limited %>%
  filter(complaint_type %in% c('Street Condition',"Traffic Signal Condition","Street Light Condition","Highway Condition",
                              "Bridge Condition","Tunnel Condition","DEP Street Condition","DEP Highway Condition"))

### limiting to three Categories
df_311_2025_limited <- df_311_2025_limited %>%
  mutate(complaint_group_custom = case_when(
    complaint_type %in% c("Street Condition", "Highway Condition", "Bridge Condition", "Tunnel Condition") ~ "road_infra_based_complaint",
    complaint_type %in% c("DEP Street Condition", "DEP Highway Condition") ~ "dep_infra_based_complaint",
    complaint_type %in% c("Traffic Signal Condition", "Street Light Condition") ~ "light_signal_based_complaint",
    TRUE ~ "other"
  ))


print(nrow(df_311_2025_limited))#52,176
## adding in zcta, inner join to keep valid zip codes
df_311_2025_limited <- df_311_2025_limited %>%  rename(zip_code = incident_zip)
df_311_2025_limited <- df_311_2025_limited %>% inner_join(crosswalk_df, by = "zip_code")
print(nrow(df_311_2025_limited)) #51,926



## Getting the total complaints with leaving gthe columns we care abouts
agg_311_2025 <- df_311_2025_limited %>%
  group_by(year_month, modzcta, complaint_group_custom, days_open_group) %>%
  summarize(complaint_count = n_distinct(unique_key), .groups = "drop")

print(head(agg_311_2025))

### I think we still need to make wider to flatten out wide-ways for regression and for smooth join with accdient data

## making the two complaint categories into one and then making them each a column
agg_311_2025 <- agg_311_2025 %>%
  mutate(complaint_categories = paste(complaint_group_custom, days_open_group, sep = "_"))|>
  select(year_month, modzcta,complaint_categories, complaint_count)

agg_311_2025_wide <- agg_311_2025 %>%
  pivot_wider(
    names_from = complaint_categories,
    values_from = complaint_count,
    values_fill = 0 )

## THis DF if for regression work to join in semi final with crash
print(head(agg_311_2025_wide))
```

```{r}
## Pulling in 2025 Data from NYC Accidents 
base_url <- "https://data.cityofnewyork.us/resource/h9gi-nx95.json"

# Same Date limits, resetting the offsets and other vars
offset <- 0
all_data <- list()
done <- FALSE

# Loop for pagination for 2025
start_date <- paste0(2025, "-01-01T00:00:00")
end_date <- paste0(2025 + 1, "-01-01T00:00:00")

#Pulling loop
while (!done) {
  query <- list(
"$where" = paste0(
    "crash_date >= '", start_date,
    "' AND crash_date < '", end_date,
    "' AND zip_code IS NOT NULL"
  ),    "$limit" = rows_per_request,
    "$offset" = offset
  )
  print(offset)
  response <- GET(base_url, query = query)
  data_chunk <- fromJSON(content(response, as = "text", encoding = "UTF-8"))

  if (length(data_chunk) == 0) {
    done <- TRUE
  } else {
    all_data <- append(all_data, list(data_chunk))
    offset <- offset + rows_per_request
    Sys.sleep(2)
  }
}
#
# # Combine and save
df_crashes_2025 <- bind_rows(all_data)


## Savign to  a file to pick up later if needed.
saveRDS(df_crashes_2025, "motor_vehicle_crashes_2025.rds")
df_crashes_2025 <- readRDS("motor_vehicle_crashes_2025.rds")
```
```{r}
## number of vehicles involved in an accident
df_crashes_2025_mod <- df_crashes_2025 %>%
  mutate(
      vehicle_1 = coalesce(vehicle_type_code1, contributing_factor_vehicle_1),
      vehicle_2 = coalesce(vehicle_type_code2, contributing_factor_vehicle_2),
      vehicle_3 = coalesce(vehicle_type_code_3, contributing_factor_vehicle_3),
      vehicle_4 = coalesce(vehicle_type_code_4, contributing_factor_vehicle_4),
      vehicle_5 = coalesce(vehicle_type_code_5, contributing_factor_vehicle_5)) %>%
  mutate(
      vehicles_involved= rowSums(!is.na(select(., vehicle_1, vehicle_2, vehicle_3, vehicle_4, vehicle_5))),
    ) %>%  select(crash_date,collision_id, borough, zip_code, number_of_persons_injured, number_of_persons_killed, vehicles_involved)

# Seeing Results
#hist(df_crashes_mod$vehicles_involved)

# There are zeros which makes no sense, so we're going to make the zeros have a 1 value. I'm assuming its a data entry error.
df_crashes_2025_mod <- df_crashes_2025_mod %>%  mutate(vehicles_involved = ifelse(vehicles_involved == 0, 1, vehicles_involved))

#formatting dates
df_crashes_2025_mod$crash_date <- ymd_hms(df_crashes_2025_mod$crash_date)


## Limiting to the columns we want to keep. 
df_crashes_2025_lim <- df_crashes_2025_mod |> select(crash_date, collision_id, borough, zip_code, number_of_persons_injured, number_of_persons_killed, vehicles_involved) 

## Enriching with zctas, the actual geogs.
print(nrow(df_crashes_2025_lim))#478,560
## adding in zcta, inner join to keep valid zip codes
df_crashes_2025_lim <- df_crashes_2025_lim %>% inner_join(crosswalk_df, by = "zip_code")
print(nrow(df_crashes_2025_lim)) #477,287
print(head(df_crashes_2025_lim))

## Extracting month year
df_crashes_2025_lim$year_month <- floor_date(df_crashes_2025_lim$crash_date, unit = "month")

## Group by to get counts
agg_crash_2025 <- df_crashes_2025_lim %>%
  group_by(year_month, modzcta) |> 
  summarize(accident_count = n_distinct(collision_id), .groups = "drop")

print(head(agg_crash_2025))

```

```{r}
##Working Data set for 2025 Test data. 

## Joining both processed datasets into 1, also adding Zip Neighborhood info from the ZCTA data 
testing_2025_data <- full_join(agg_crash_2025,  agg_311_2025_wide,  by = c("year_month", "modzcta"))

### Last Mile clean up for Borough and Zip 

print(nrow(testing_2025_data)) # 12,634 rows 

# We need a zip code value
testing_2025_data <- testing_2025_data |>  filter(!(is.na(modzcta)))


print(nrow(testing_2025_data)) # 12,634  rows  (dropped a null row for zip)


## Filling in the nulls w2ith zeros
testing_2025_data_full <- testing_2025_data
testing_2025_data_full[is.na(testing_2025_data_full)] <- 0
print(head(testing_2025_data_full))
```

``` {r}
#write.csv(semifinal_data_full, "data621_finalproject_semifinal_data.csv", row.names = FALSE)
```
