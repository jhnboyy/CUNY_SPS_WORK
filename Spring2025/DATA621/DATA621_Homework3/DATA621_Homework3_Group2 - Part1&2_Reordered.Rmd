---
title: "DATA621_Homework3_Group2"
author: "Group2"
date: "`r Sys.Date()`"
output:
    html_document:
      code_folding: hide
      toc: true
      toc_float: false
      toc_depth: 5
      number_sections: false
      highlight: pygments
      theme: cerulean
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(dplyr)
library(ggplot2)
#library(mice)
library(GGally)
#library(moments)
library(tidyr)
library(tidyverse)
library(MASS)
```

## Overview
In this homework assignment, you will explore, analyze and model a data set containing information on crime for various neighborhoods of a major city. Each record has a response variable indicating whether or not the crime rate is above the median crime rate (1) or not (0). Your objective is to build a binary logistic regression model on the training data set to predict whether the neighborhood will be at risk for high crime levels. You will provide classifications and probabilities for the evaluation data set using your binary logistic regression model. You can only use the variables given to you (or, variables that you derive from the variables provided). Below is a short description of the variables of interest in the data set:
  • zn: proportion of residential land zoned for large lots (over 25000 square feet) (predictor variable)
  • indus: proportion of non-retail business acres per suburb (predictor variable)
  • chas: a dummy var. for whether the suburb borders the Charles River (1) or not (0) (predictor variable)
  • nox: nitrogen oxides concentration (parts per 10 million) (predictor variable)
  • rm: average number of rooms per dwelling (predictor variable)
  • age: proportion of owner-occupied units built prior to 1940 (predictor variable)
  • dis: weighted mean of distances to five Boston employment centers (predictor variable)
  • rad: index of accessibility to radial highways (predictor variable)
  • tax: full-value property-tax rate per $10,000 (predictor variable)
  • ptratio: pupil-teacher ratio by town (predictor variable)
  • lstat: lower status of the population (percent) (predictor variable)
  • medv: median value of owner-occupied homes in $1000s (predictor variable)
  • target: whether the crime rate is above the median crime rate (1) or not (0) (response variable) 
  
## Deliverables:
  • A write-up submitted in PDF format. Your write-up should have four sections. Each one is described below. You may assume you are addressing me as a fellow data scientist, so do not need to shy away from technical details.
  • Assigned prediction (probabilities, classifications) for the evaluation data set. Use 0.5 threshold. Include your R statistical programming code in an Appendix.

### 1. DATA EXPLORATION (25 Points)
Describe the size and the variables in the crime training data set. Consider that too much detail will cause a manager to lose interest while too little detail will make the manager consider that you aren’t doing your job. Some suggestions are given below. Please do NOT treat this as a check list of things to do to complete the assignment. You should have your own thoughts on what to tell the boss. These are just ideas.
  a. Mean / Standard Deviation / Median
  b. Bar Chart or Box Plot of the data
  c. Is the data correlated to the target variable (or to other variables?)
  d. Are any of the variables missing and need to be imputed/“fixed”? 
  
#### DATA EXPLORATION WRITE UP
The training dataset consists of 466 observations and 14 variables, including 13 predictors and a binary response variable. The binary response variable, target, indicates that a neighborhood's crime rate exceeds the citywide median (a value of 1) or not ( a value of 0). A null-check confirmed that there are no missing values in the dataset, so no imputation or data cleansing was necessary at this stage.

Basic summary statistics reveal that the variables like zn, tax, and age exhibit wide ranges seemingly from 0 to 100 or more. Columns such as indus, nox, lstat, medv, rad and dis have smaller ranges. Looking specifically at some variable, zn, which is the proportion of residential land zoned for large lots, ranges from 0 to 100. The data has a median of 0 and a mean of 11.58. This shows that many neighborhoods have minimal or no large-lot zoning. Another variable, age , which shows the proportion of homes built before 1940, has a mean of 68.37 and a median of 77.15, suggesting a majority of the housing stock is relatively old. 

Leveraging both box plots for each variable compared to the target variable, along with a larger ggpairs plot, there were relationships between the data columns observed. 

Firstly, take a look at the ggpairs plot below. This plot revealed several meaningful correlations and patterns. A strong negative correlation exists between zn and both indus and nox. This implies areas zoned for spacious residential use are less likely to host industrial development and experience fewer nitrogen oxide emissions. The zn variable also negatively correlates with age. This could indicate larger residential lots are more common in newer developments. Indus, nox, and age are positively correlated with one another, which could mean that older neighborhoods are more industrial and environmentally burdened. Additionally, a strong negative relationship between the age and rm variables was seen, implying that older buildings are generally smaller than newer ones. The distance to employment centers, covered by the dis variable, negatively correlates with nox, age, and indus. The dis variable, however, has a positive relationship with zn. This may mean that larger residential zoning, are more suburban type developments not near industrialized more urban / downtown environments. Lastly, the lstat variable, which looks at the proportion of lower status of the population, is negatively related to zn, rm and dis. Lstat is positively correlated with indus, nox, and age. This makes sense as older, less industrialized, more suburban areas would tend to be of higher status.


![GGPairs Plot Training Data](https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA621/DATA621_Homework3/images/ggpairs_plot.png)
Now looking at the box plots (below) charted for each variable to gauge the relationship to the target variable of crime level, there were take aways here. These takeways can be seen in the following summary table: 

| Variable | Relationship to Crime (Target = 1) | Insight |
|----------|------------------------------------|---------|
| `zn`     | Lower values → Higher crime        | Less residential zoning is linked to high crime |
| `indus`  | Higher values → Higher crime       | More industrial areas tend to have higher crime |
| `nox`    | Higher values → Higher crime       | Pollution correlates with higher crime rates |
| `rm`     | No clear pattern                   | Number of rooms not strongly associated |
| `age`    | Higher values → Higher crime       | Older buildings correlate with higher crime |
| `dis`    | Lower values → Higher crime        | Areas closer to employment centers have more crime |
| `rad`    | Higher values → Higher crime       | Greater radial highway access correlated to higher crime |
| `tax`    | Higher values → Higher crime       | High tax correlated higher crime. |
| `ptratio`| Slightly higher values → Higher crime | Weak trend, possibly related to school quality |
| `lstat`  | Slightly higher values → Higher crime       | Larger lower status proportion linked to higher crime |
| `medv`   | Slightly lower values → Higher crime        | Lower median owner-occupied home values associated with high crime |


![Box Plot for zn and Target Variable](https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA621/DATA621_Homework3/images/Boxplotzn_plot.png)
![Box Plot for indus and Target Variable](https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA621/DATA621_Homework3/images/Boxplotindus_plot.png)
![Box Plot for nox and Target Variable](https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA621/DATA621_Homework3/images/Boxplotnox_plot.png)
![Box Plot for rm and Target Variable](https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA621/DATA621_Homework3/images/Boxplotrm_plot.png)
![Box Plot for age and Target Variable](https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA621/DATA621_Homework3/images/Boxplotage_plot.png)
![Box Plot for dis and Target Variable](https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA621/DATA621_Homework3/images/Boxplotdis_plot.png)
![Box Plot for rad and Target Variable](https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA621/DATA621_Homework3/images/Boxplotrad_plot.png)
![Box Plot for tax and Target Variable](https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA621/DATA621_Homework3/images/Boxplottax_plot.png)
![Box Plot for ptratio and Target Variable](https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA621/DATA621_Homework3/images/Boxplotptratio_plot.png)
![Box Plot for lstat and Target Variable](https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA621/DATA621_Homework3/images/Boxplotlstat_plot.png)
![Box Plot for medv and Target Variable](https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA621/DATA621_Homework3/images/Boxplotmedv_plot.png)

In summation for data exploration, the target variable covering crime rates seems to have meaningful relationships with several of the predictors in the dataset. Additionally, non-target variables have relationships with one another that should be considered for processing and before modeling. The suggestions from this exploration hint that zoning, environmental quality, infrastructure access, and socioeconomic factors could all play a role in predicting crime levels.

#### Reading in Data
```{r reading_in_data}
## Pushed the small amount of data to git, so reading in from there.

crime_eval_mod <- read.csv("https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA621/DATA621_Homework3/crime-evaluation-data_modified.csv")

crime_train_mod <- read.csv("https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA621/DATA621_Homework3/crime-training-data_modified.csv")
```

### Exploring the Training Data
```{r exploring_the_training_data}
## Summary of the Training Data
print(summary(crime_train_mod))

# Double Checking for NA 
print(colSums(is.na(crime_train_mod))) # No nulls to remove/ impute.
print(nrow(crime_train_mod)) #466 rows


## Looping throuhg all of the continous columns, and plotting with target on the x axis. 
for (col in c(colnames(crime_train_mod |> dplyr::select(-target,-chas)))) {
  #print(col)
  box_plot <- ggplot(crime_train_mod, aes(x = factor(target), y = .data[[col]])) +
    geom_boxplot(outlier.shape = 1) +
    labs(
      x = "Target (0 = No, 1 = Yes)",
      y = col,
      title = paste("Boxplot of", col, "by Target")
    )
  #print(box_plot)
  #ggsave(paste0("images/Boxplot",col,"_plot.png"), plot = box_plot, units = "in", dpi = 300)

}

## Based on these box plots, target's relationship with other variables seems as follows: 
#- zn: Nearly all of the higher zoning values correlate with 0 values of target. Lower Crime, higher zoning value
#- indus: the higher values for indus generally correlate with higher crime target values. Some exceptions.
#- nox: higher nox values correlated with high crime target vals. 
#- rm: not much of an outlying pattern here.
#- age: slight trend of higher age of area, higher crime target values.
#- dis: higher values seem to be lower crime, some exceptions.
#- rad: higher values associated with higher crime.
#- tax: higher values associated with higher crime, some exceptions.
#- ptratio: very slight correlation, not too definitive. Higher ptratio, maybe higher crime.
#- lstat: slight correlation, high crime with higher lstat values
#- medv: weak correlation lower crime, hiwher medv number. 

```

#### GGPairs Visual for Training Data
```{r ggpairs, fig.width = 15, fig.height =20}
## Taking a look at how the variables relate to one another via ggpairs.

#p <- 
  ggpairs(crime_train_mod,progress = FALSE) +theme_minimal(base_size=9) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
                                      strip.text.x = element_text(angle = 90, hjust = 1),
                                     strip.text.y = element_text(angle = 0, hjust = 1))

## Saving Image for reference in PDF #
#ggsave("images/ggpairs_plot.png", plot = p, width = 15, height = 20, units = "in", dpi = 300)

## The takeaways that i see for the GGpairs chart for iteration / relationships between variables:
##  - There is a negative relationship between zn(Large residential Lot Zoned Land) and indus (proportion of non retail businesses). Makes sense as the larger the residential land
##    zoning is, the less likely there will be non retail / industrial type businesses also prominent in the area. 
##  - Similar negative relationship for NOX and ZN, which makes sense because there is also a direct relationship with indus and nox. Im assuming NOX is an industrial emission.
##  - Negative relationship btwn zn and age, which would imply that larger residential type living zone is a newer development. Where older development buildings are zoned smaller
##  - Direct relationship with age and nox as well as indus, which i would guess is that neighborhoods with older building areas, have more industrial development and nox emission
##  - Steep neg. relationship with age and rm. Older buildings smaller so older buildings, have less rooms.
##  - Age has negative relationship with zn, and positive one with indus
##  - dis column, the weighted dist. to employment centers, has a negative relationship with: nox, age, indus. While dis has a positive relationship with zn.
##  - lstat, lower status population, has neg. relationship with zn, rm and dis. While lstat has pos. relationship with: age, indus, nox, and a slight one with ptratio
##  - medv, med. value of owner-occupied home, has a neg. relationship with: indus, nox, lstat, and slight neg. with age. medv has positive rel. with: dis, rm, zn.

```

#### Correlation Matrix Check 

```{r correlation_matrix_check}
# Cor Matrics with no dummy var chas or binary target 
cor_matrix <- cor(crime_train_mod |> dplyr::select(-target,-chas), use = "complete.obs")
print(cor_matrix)
# This tracks with what i outlined in my notes on the ggpairs plot.

## Looking here in order to find potential agg index variables to make. 
## Potential colinear type relationships here: 
# indus -> nox, - dis
# nox -> -dis, age (age gives diff info though)
# age -> dis
# rad -> tax (super high correlation)
# lstat -> - medv
```

</br>

### 2. DATA PREPARATION (25 Points)
Describe how you have transformed the data by changing the original variables or creating new variables. If you did transform the data or create new variables, discuss why you did this. Here are some possible transformations.
  a. Fix missing values (maybe with a Mean or Median value)
  b. Create flags to suggest if a variable was missing
  c. Transform data by putting it into buckets
  d. Mathematical transforms such as log or square root (or, use Box-Cox)
  e. Combine variables (such as ratios or adding or multiplying) to create new variables 
  
</br>

#### Check For Missing Values

We check for missing values in both training and evaluation crime datasets by inspecting the extent of missing data in each by computing and plotting the number of missing values per variable. There are **no missing values** in our training and evaluation datasets and no further treatment is needed.

Logistic regression assumes that the input data is complete and does not handle missing values natively. If missing values are not addressed, this can lead to biased parameter estimates, reduced model accuracy, and invalid inference. Ignoring missing values might exclude important observations, potentially distorting the relationship between predictors and the binary outcomes. Also pattern of missingness may contain important information about the underlying data-generating process that could be relevant to the crime prediction logistic regression model. We can use imputation, deletion, or others methods to model coefficients accurately,

```{r}
library(ggplot2)
library(gridExtra)

# Load datasets
crime_eval_mod <- read.csv("https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA621/DATA621_Homework3/crime-evaluation-data_modified.csv")
crime_train_mod <- read.csv("https://raw.githubusercontent.com/jhnboyy/CUNY_SPS_WORK/refs/heads/main/Spring2025/DATA621/DATA621_Homework3/crime-training-data_modified.csv")

# Compute missing value counts for training data
missing_train <- sapply(crime_train_mod, function(x) sum(is.na(x)))
missing_train_df <- data.frame(Variable = names(missing_train), MissingCount = missing_train)

# Compute missing value counts for evaluation data
missing_eval <- sapply(crime_eval_mod, function(x) sum(is.na(x)))
missing_eval_df <- data.frame(Variable = names(missing_eval), MissingCount = missing_eval)

# Create plots
plot_train <- ggplot(missing_train_df, aes(x = reorder(Variable, MissingCount), y = MissingCount)) +
  geom_bar(stat = "identity", fill = "red") +
  labs(title = "Missing Values: Training Data", x = "Variable", y = "Count") +
  coord_flip() +
  theme_minimal()

plot_eval <- ggplot(missing_eval_df, aes(x = reorder(Variable, MissingCount), y = MissingCount)) +
  geom_bar(stat = "identity", fill = "red") +
  labs(title = "Missing Values: Evaluation Data", x = "Variable", y = "Count") +
  coord_flip() +
  theme_minimal()

grid.arrange(plot_train, plot_eval, ncol = 2)


```

</br>

#### Address Skewness

We addresses skewness in a set of variables by applying a Box‑Cox transformation. For each skewed variable =, **“zn”, “dis”, “lstat”, “medv"**, we adjust the data to ensure all values are positive by adding an offset if necessary and compute the optimal lambda parameter using the Box‑Cox method.  The computed lambda is used to transform the variable in both the training and evaluation datasets.

Our tranformation result in "medv" (median home value) and "lstat" (% lower status population) are positively skewed but become more symmetric with λ=0.2; "dis" (distance to employment centers) is also right-skewed but transforms well with λ=-0.1; and "zn" (residential land zoning) has an extreme positive skew that becomes more balanced with λ=-0.9.

Transforming skewed variables is important in logistic regression because the models perform better when the predictors are symmetrically distributed. Skewness can lead to unstable coefficient estimates, affect the interpretability of model parameters, and ultimately reduce predictive performance. The Box‑Cox transformation helps to stabilize variance and reduce the influence of outliers.

```{r}
library(MASS)
library(gridExtra)
library(ggplot2)

# Identify skewed variables
skewed_vars <- c("zn", "dis", "lstat", "medv")

# Prepare list for plots
transformation_plots <- list()

# Loop through each skewed variable
for (var in skewed_vars) {
  # Adjust training data so all values are positive
  x <- crime_train_mod[[var]]
  offset <- pmax(1 - min(x, na.rm = TRUE), 0)
  x_adj <- x + offset
  
  # Compute Box-Cox lambda
  bc <- boxcox(x_adj ~ 1, lambda = seq(-2, 2, by = 0.1), plotit = FALSE)
  lambda <- bc$x[which.max(bc$y)]
  
  # Apply Box-Cox transformation to the training data
  train_trans <- (x_adj^lambda - 1) / lambda
  
  # Adjust and transform evaluation data
  x_eval <- crime_eval_mod[[var]]
  offset_eval <- pmax(1 - min(x_eval, na.rm = TRUE), 0)
  x_eval_adj <- x_eval + offset_eval
  eval_trans <- (x_eval_adj^lambda - 1) / lambda
  
  # Save transformed variables
  crime_train_prep <- crime_train_mod
  crime_train_prep[[paste0(var, "_trans")]] <- train_trans
  crime_eval_prep <- crime_eval_mod
  crime_eval_prep[[paste0(var, "_trans")]] <- eval_trans
  
  # Create the "before" density plot
  before_plot <- ggplot(crime_train_mod, aes(x = .data[[var]])) +
    geom_density(fill = "steelblue", alpha = 0.7) +
    labs(title = paste("Before:", var),
         x = var, y = "Density") +
    theme_minimal(base_size = 9)
  
  # Create the "after" density plot
  after_plot <- ggplot(crime_train_prep, aes(x = .data[[paste0(var, "_trans")]])) +
    geom_density(fill = "gray", alpha = 0.7) +
    labs(title = paste("After: Box-Cox (λ =", round(lambda, 2), ") Transform"),
         x = paste0(var, "_trans"), y = "Density") +
    theme_minimal(base_size = 9)
  
  # Arrange the before and after plots side by side and store in the list
  transformation_plots[[var]] <- grid.arrange(before_plot, after_plot, ncol = 2)
}

```




</br>

#### Check for Multicollinearity

We evaluate multicollinearity among predictor variables using the Variance Inflation Factor (VIF). After fitting a logistic regression model with all available predictors, we compute the VIF for each variable to assess how much the variance of each estimated coefficient is inflated due to linear relationships with other predictors. A VIF value greater than 5 indicates high (and potentially problematic) multicollinearity.

Multicollinearity can impact the stability and interpretability of logistic regression models. When predictors are highly correlated,estimating their individual contributions to the outcome variable is difficult because their effects may be confounded. This can lead to inflated standard errors, unreliable coefficient estimates, and misleading inference, In our training date,  **rm (average number of rooms)** and **medv (median home value)** have VIF values of 5.81 and 8.12, respectively; these variables may be collinear with others in the model. To address collinearity, we may need to drop or combine variables to create an interpretable logistic regression model.



```{r}
library(car)

# Check for multicollinearity with Variance Inflation Factor
initial_model <- glm(target ~ . , data=crime_train_mod, family=binomial)
vif_results <- vif(initial_model)
print("Variance Inflation Factors:")
print(vif_results)

# Display variables with high VIF (>5 has high multicollinearity)
high_vif <- vif_results[vif_results > 5]

print("Variables with high multicollinearity:")
print(high_vif)

# Visualize the VIF results
vif_df <- data.frame(
  Variable = names(vif_results),
  VIF = as.numeric(vif_results))
vif_df <- vif_df[order(-vif_df$VIF),]

ggplot(vif_df, aes(x = reorder(Variable, VIF), y = VIF)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  geom_hline(yintercept = 5, linetype = "dashed", color = "orange") +
  labs(title = "Variance Inflation Factors (VIF) for Predictor Variables",
       subtitle = "VIF > 5 (High Multicollinearity)",
       x = "Variables", y = "VIF") +
  coord_flip() +
  theme_minimal()
```

</br>

#### Create Composite Predictors

Here we construct and visualize four composite combined indices, **env_index, ses_index, urban_index, and infra_index** that aggregate predictors into interpretable combinations related to environmental quality, socioeconomic status, urban density, and infrastructure access. The indices are created by combining variables reflecting conceptual groupings that influence crime. The environmental index combines industrial activity, pollution levels, and zoning; the socioeconomic index combines wealth, housing value and taxation. We compare the new composite predictors to the binary crime outcome (target: 0 = Low Crime, 1 = High Crime) to see how each composite measure differs between high- and low-crime areas.  

Composite (combined) indices  are important for interpretability and dimensionality reduction in logistic regression modeling. Rather than including many correlated predictors individually, These composites simplify the structure of the data while preserving essential information. This approach directly addresses the multicollinearity issue identified in the previous VIF analysis by combining correlated predictors ('rm' and 'medv') into conceptually coherent aggregate predictors.  

The boxplots show that each index clearly separates the two crime categories: high-crime areas are associated with higher values of env_index, infra_index, ses_index, and urban_index. They suggest that poor environmental conditions, increased infrastructure strain, lower socioeconomic status, and high urban density are linked to higher crime. The clear separation between high and low crime areas suggests these composite variables capture neighborhood characteristics that strongly associate with crime rates, potentially improving model performance


```{r}
library(dplyr)
library(tidyr)
library(ggplot2)

# Create composite indices
crime_train_prep <- crime_train_mod %>%
  mutate(
    # Environmental quality index
    env_index = scale(indus) + scale(nox) - scale(zn),
    
    # Socioeconomic status index
    ses_index = scale(lstat) - scale(medv) + scale(tax),
    
    # Urban density index
    urban_index = scale(age) - scale(dis) - scale(rm),
    
    # Infrastructure access index
    infra_index = scale(rad) + scale(ptratio))

# Add the indices to evaluation data
crime_eval_prep <- crime_eval_mod %>%
  mutate(
    env_index = scale(indus) + scale(nox) - scale(zn),
    ses_index = scale(lstat) - scale(medv) + scale(tax),
    urban_index = scale(age) - scale(dis) - scale(rm),
    infra_index = scale(rad) + scale(ptratio))

# Visualize the relationship between indices and target variable
indices_long <- crime_train_prep %>%
  dplyr::select(target, env_index, ses_index, urban_index, infra_index) %>%
  tidyr::pivot_longer(cols = -target, names_to = "Index", values_to = "Value")

ggplot(indices_long, aes(x = factor(target), y = Value, fill = factor(target))) +
  geom_boxplot() +
  facet_wrap(~ Index, scales = "free_y") +
  labs(title = "Composite Indices vs Crime Rate Target",
       x = "Target (0 = Low Crime, 1 = High Crime)",
       y = "Index Value",
       fill = "Crime Rate") +
  scale_fill_manual(values = c("0" = "steelblue", "1" = "gray"),
                    labels = c("0" = "Low Crime", "1" = "High Crime")) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

#### Create Ratio Variables

We create three new ratio-based features, **tax_value_ratio, edu_resource_ratio, and urban_density_ratio** and plot how they differ between low-crime (target=0) and high-crime (target=1) areas. The boxplots clearly show that all three ratios strongly differentiate between high and low crime areas, with high-crime neighborhoods consistently showing higher ratio values than low-crime areas. If tax_value_ratio are higher for high-crime areas, it may suggest that high property taxes relative to home values correlate with increased crime rates. Similarly, differences in edu_resource_ratio or urban_density_ratio between low- and high-crime groups indicate that educational resources or urban development factors may also play a role


By combining existing features into ratios, we capture potentially meaningful relationships in a single measure. These ratio metrics can yield more interpretable predictors for logistic regression, as they may capture underlying factors more effectively than raw features alone. Including these ratio features in a logistic regression model can improve predictive power and interpretability. Ratios may capture theoretically meaningful relationships better than individual variables alone.  They can also address multicollinearity by combining related variables into single predictors while preserving their interactive relationship.

```{r}

library(dplyr)
library(tidyr)
library(ggplot2)

# Create ratio variables
crime_train_prep <- crime_train_prep %>%
  mutate(
    # Economic burden ratio measure tax relative to housing value
    tax_value_ratio = tax / medv,
    
    # Educational resources ratio measures classroom size with neighborhood status
    edu_resource_ratio = ptratio / (1 - (lstat/100)),
    
    # Urbanization ratio measures age of buildings relative to distance from employment centers
    urban_density_ratio = age / dis)

# Add ratios to the evaluation dataset
crime_eval_prep <- crime_eval_prep %>%
  mutate(
    tax_value_ratio = tax / medv,
    edu_resource_ratio = ptratio / (1 - (lstat/100)),
    urban_density_ratio = age / dis)

# Plot relationship between ratio variables and target
ratio_long <- crime_train_prep %>%
  dplyr::select(target, tax_value_ratio, edu_resource_ratio, urban_density_ratio) %>%
  tidyr::pivot_longer(cols = -target, names_to = "Ratio", values_to = "Value")

ggplot(ratio_long, aes(x = factor(target), y = Value, fill = factor(target))) +
  geom_boxplot() +
  facet_wrap(~ Ratio, scales = "free_y") +
  labs(title = "Ratio Variables vs  Crime Rate",
       x = "Target (0 = Low Crime, 1 = High Crime)",
       y = "Ratio Value",
       fill = "Crime Rate") +
  scale_fill_manual(values = c("0" = "steelblue", "1" = "gray"),
                    labels = c("0" = "Low Crime", "1" = "High Crime")) +
  theme_minimal() +
  theme(legend.position = "bottom")

```
#### Standardize and Scale Predictors

We standardize the numeric values **"indus", "nox", "age", "tax", "lstat"** by subtracting the mean and dividing by the standard deviation. By applying the same mean and standard deviation to both the training and evaluation datasets, we ensure that all numeric features share a comparable scale and distribution across both datasets. The density plots indicate that the original and standardized distributions have the same general shape, but the standardized versions are shifted and rescaled to center around zero -- with a standard deviation of one.

This is particularly useful for logistic regression as standardized features can help with model convergence, improve interpretability of coefficients, and prevent any single variable from dominating the model solely because of its larger numeric range. Standardized predictors contribute proportionally to the logistic regression model based on their information content rather than their  scale, preventing variables with larger ranges from dominating those with smaller ranges.


```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(recipes)

# Identify numeric variables for scaling
num_vars <- names(crime_train_prep)[sapply(crime_train_prep, is.numeric)]
num_vars <- num_vars[!num_vars %in% c("target", "chas")]

# Create standardization recipe
standardization_recipe <- recipe(target ~ ., data = crime_train_prep) %>%
  step_normalize(all_of(num_vars)) %>%
  prep(training = crime_train_prep)

# Update datasets
crime_train_prep_scaled <- bake(standardization_recipe, new_data = crime_train_prep)
crime_eval_prep_scaled <- bake(standardization_recipe, new_data = crime_eval_prep)

# Compare original and standardized distributions
orig_vars <- c("indus", "nox", "age", "tax", "lstat")
scaled_vars <- orig_vars

# Create dataframe for visualization
scale_compare <- bind_rows(
  crime_train_mod %>%
    dplyr::select(all_of(orig_vars)) %>%
    pivot_longer(cols = everything(), 
                 names_to = "Variable", 
                 values_to = "Value") %>%
    mutate(Type = "Original"),
  
  # Standardized data
  crime_train_prep_scaled %>%
    dplyr::select(all_of(scaled_vars)) %>%
    pivot_longer(cols = everything(), 
                 names_to = "Variable", 
                 values_to = "Value") %>%
    mutate(Type = "Standardized"))

# Plot effect of standardization
ggplot(scale_compare, aes(x = Value, fill = Type)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ Variable, scales = "free", ncol = 2) +
  labs(title = "Effect of Standardization on Variable Distributions",
       x = "Value", y = "Density") +
  scale_fill_manual(values = c("Original" = "orange", "Standardized" = "blue")) +
  theme_minimal() +
  theme(legend.position = "bottom")
```



### 3. BUILD MODELS (25 Points)
Using the training data, build at least three different binary logistic regression models, using different variables (or the same variables with different transformations). You may select the variables manually, use an approach such as Forward or Stepwise, use a different approach, or use a combination of techniques. Describe the techniques you used. If you manually selected a variable for inclusion into the model or exclusion into the model, indicate why this was done. Be sure to explain how you can make inferences from the model, as well as discuss other relevant model output. Discuss the coefficients in the models, do they make sense? Are you keeping the model even though it is counter intuitive? Why? The boss needs to know. 





### 4. SELECT MODELS (25 Points)
Decide on the criteria for selecting the best binary logistic regression model. Will you select models with slightly worse performance if it makes more sense or is more parsimonious? Discuss why you selected your model. 
  • For the binary logistic regression model, will you use a metric such as log likelihood, AIC, ROC curve,etc.? Using the training data set, evaluate the binary logistic regression model based on (a) accuracy, (b)classification error rate, (c) precision, (d) sensitivity, (e) specificity, (f) F1 score, (g) AUC, and (h)confusion matrix. Make predictions using the evaluation data set.







## Appendix A - Code Work



